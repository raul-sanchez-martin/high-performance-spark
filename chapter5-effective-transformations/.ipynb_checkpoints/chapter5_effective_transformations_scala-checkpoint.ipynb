{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Effective Transformations\n",
    "\n",
    "In this chapter, we will explore advance features of RDDs in order to perform effective transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Object Creation\n",
    "\n",
    "Minimizing the number of objects that our program creates is a great way to optimize our calculations. We are going to visit some examples which display the same functionalities but with a different number of objects created during its execution. In particular, we are going to consider a intial data where we have the report cards of different instructors for different pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recordCars = ParallelCollectionRDD[0] at parallelize at <console>:27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at <console>:27"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val recordCars = sc.parallelize(Array((\"instructor1\", \"this is a happy panda\"),\n",
    "                                      (\"instructor1\", \"this is a very happy panda\"),\n",
    "                                      (\"instructor2\", \"good\"),\n",
    "                                      (\"instructor2\", \"happy\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over that data, we want to calculate, for each instructor: the longest woard, the mentions of happy and the average words of their reporst. For doing so, we are going to use different Aggregator classes, in which the number of objects created are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class ReportCardMetrics\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class ReportCardMetrics(longestWord: Int, happyMentions: Int, averageWords: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MetricsCalculator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MetricsCalculator(val totalWords: Int, \n",
    "                        val longestWord: Int,\n",
    "                        val happyMentions: Int,\n",
    "                        val numberReportCards: Int) extends Serializable {\n",
    "    \n",
    "    def seqenceOp(reportCardContent: String): MetricsCalculator = {\n",
    "        \n",
    "        val words = reportCardContent.split(\" \")\n",
    "        val tW = words.length\n",
    "        val hM = words.count(w => w.toLowerCase.equals(\"happy\"))\n",
    "        val lW = words.map(w => w.length).max\n",
    "        \n",
    "        new MetricsCalculator(tW + totalWords, Math.max(longestWord, lW),\n",
    "                             hM + happyMentions, numberReportCards + 1)\n",
    "        \n",
    "        \n",
    "    }\n",
    "    \n",
    "    def compOp(other: MetricsCalculator): MetricsCalculator = {\n",
    "        \n",
    "        new MetricsCalculator(this.totalWords + other.totalWords,\n",
    "                             Math.max(this.longestWord, other.longestWord),\n",
    "                             this.happyMentions + other.happyMentions,\n",
    "                             this.numberReportCards + other.numberReportCards)\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    def toReportCardMetrics = ReportCardMetrics(longestWord, happyMentions, \n",
    "                                                totalWords.toDouble/numberReportCards)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(instructor1,ReportCardMetrics(5,2,5.5)), (instructor2,ReportCardMetrics(5,1,1.0))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recordCars.aggregateByKey(zeroValue = new MetricsCalculator(0,0,0,0))(\n",
    "                         seqOp = ((reportCardMetrics, reportCardText) => reportCardMetrics.seqenceOp(reportCardText)),\n",
    "                         combOp = ((x, y) => x.compOp(y))).map(x => (x._1, x._2.toReportCardMetrics)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MetricsCalculatorReuseObjects\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MetricsCalculatorReuseObjects(var totalWords: Int, \n",
    "                                    var longestWord: Int,\n",
    "                                    var happyMentions: Int,\n",
    "                                    var numberReportCards: Int) extends Serializable {\n",
    "    \n",
    "    def seqenceOp(reportCardContent: String): this.type = {\n",
    "        \n",
    "        val words = reportCardContent.split(\" \")\n",
    "        totalWords += words.length\n",
    "        happyMentions += words.count(w => w.toLowerCase.equals(\"happy\"))\n",
    "        longestWord = Math.max(longestWord, words.map(w => w.length).max)\n",
    "        numberReportCards += 1\n",
    "        \n",
    "        val lW = words.map(w => w.length).max\n",
    "        \n",
    "        this\n",
    "        \n",
    "        \n",
    "    }\n",
    "    \n",
    "    def compOp(other: MetricsCalculatorReuseObjects): this.type = {\n",
    "        \n",
    "        totalWords += other.totalWords\n",
    "        longestWord = Math.max(longestWord, other.longestWord)\n",
    "        happyMentions += other.happyMentions\n",
    "        numberReportCards += other.numberReportCards\n",
    "        \n",
    "        this\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    def toReportCardMetrics = ReportCardMetrics(longestWord, happyMentions, \n",
    "                                                totalWords.toDouble/numberReportCards)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(instructor1,ReportCardMetrics(5,2,5.5)), (instructor2,ReportCardMetrics(5,1,1.0))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recordCars.aggregateByKey(zeroValue = new MetricsCalculatorReuseObjects(0,0,0,0))(\n",
    "                         seqOp = ((reportCardMetrics, reportCardText) => reportCardMetrics.seqenceOp(reportCardText)),\n",
    "                         combOp = ((x, y) => x.compOp(y))).map(x => (x._1, x._2.toReportCardMetrics)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MetricsCalculatorArrays\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MetricsCalculatorArrays(val totalWordsIndex:Int = 0, \n",
    "                              val longestWordIndex:Int = 1,\n",
    "                              val happyMentionsIndex:Int = 2,\n",
    "                              val numberReportCardsIndex:Int = 3) extends Serializable {\n",
    "    \n",
    "    def seqenceOp(reportCardMetrics: Array[Int], reportCardContent: String): Array[Int] = {\n",
    "        \n",
    "        val words = reportCardContent.split(\" \")\n",
    "        \n",
    "        reportCardMetrics(totalWordsIndex) += words.length\n",
    "        reportCardMetrics(longestWordIndex) = Math.max(reportCardMetrics(longestWordIndex), \n",
    "                                                       words.map(w => w.length).max)\n",
    "        reportCardMetrics(happyMentionsIndex) += words.count(w => w.toLowerCase.equals(\"happy\"))\n",
    "        reportCardMetrics(numberReportCardsIndex) += 1\n",
    "        \n",
    "        reportCardMetrics\n",
    "        \n",
    "    }\n",
    "    \n",
    "    def compOp(x: Array[Int], y: Array[Int]): Array[Int] = {\n",
    "        \n",
    "        x(totalWordsIndex) += y(totalWordsIndex)\n",
    "        x(longestWordIndex) = Math.max(x(longestWordIndex), y(longestWordIndex))\n",
    "        x(happyMentionsIndex) += y(happyMentionsIndex)\n",
    "        x(numberReportCardsIndex) += y(numberReportCardsIndex)\n",
    "        \n",
    "        x\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    def toReportCardMetrics(x: Array[Int]) = {\n",
    "        ReportCardMetrics(x(longestWordIndex), \n",
    "                          x(happyMentionsIndex), \n",
    "                          x(totalWordsIndex).toDouble/x(numberReportCardsIndex))   \n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:31: error: value seqenceOp is not a member of Array[Int]\n",
       "                                seqOp = ((reportCardMetrics, reportCardText) => reportCardMetrics.seqenceOp(reportCardText)),\n",
       "                                                                                                  ^\n",
       "<console>:32: error: value compOp is not a member of Array[Int]\n",
       "                                combOp = ((x, y) => x.compOp(y))).map(x => (x._1, x._2.toReportCardMetrics)).collect()\n",
       "                                                      ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recordCars.aggregateByKey(zeroValue = Array(0,0,0,0))(\n",
    "                         seqOp = ((reportCardMetrics, reportCardText) => reportCardMetrics.seqenceOp(reportCardText)),\n",
    "                         combOp = ((x, y) => x.compOp(y))).map(x => (x._1, x._2.toReportCardMetrics)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Operations\n",
    "\n",
    "In this section, we include some examples of set operations that can be done in Spark highighting some peculiarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substract example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddA = ParallelCollectionRDD[5] at parallelize at <console>:27\n",
       "rddB = ParallelCollectionRDD[6] at parallelize at <console>:28\n",
       "subtraction = MapPartitionsRDD[10] at subtract at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at subtract at <console>:29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddA = sc.parallelize(Array(1,2,3,4,4,4,4))\n",
    "val rddB = sc.parallelize(Array(3,4))\n",
    "val subtraction = rddA.subtract(rddB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtraction.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(subtraction.count() < rddA.count() - rddB.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersection example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intersection = MapPartitionsRDD[16] at intersection at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val intersection = rddA.intersection(rddB)\n",
    "intersection.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "union = UnionRDD[17] at union at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 4, 4, 4, 3, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val union = rddA.union(rddB)\n",
    "union.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(!rddA.collect().sorted.sameElements(union.collect().sorted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Setup Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Panda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Panda(id: Int, zip: Int, pt: String, happy: Boolean, attributes: Array[Double])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandasRDD = ParallelCollectionRDD[18] at parallelize at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[18] at parallelize at <console>:29"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pandasRDD = sc.parallelize(Seq(Panda(1, 11000, \"giant\", false, Array(0.2, 0.8)),\n",
    "                                   Panda(2, 11000, \"small\", false, Array(0.3, 0.1)),\n",
    "                                   Panda(3, 13000, \"small\", true, Array(0.9, 0.7)),\n",
    "                                   Panda(4, 13000, \"medium\", false, Array(0.5, 0.4)),\n",
    "                                   Panda(5, 18000, \"medium\", true, Array(0.7, 0.1)),\n",
    "                                   Panda(6, 18000, \"giant\", true, Array(0.1, 0.7)),\n",
    "                                   Panda(7, 18000, \"small\", true, Array(0.3, 0.9))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Variables\n",
    "\n",
    "#### Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "invalidPandasIds = Array(2, 7)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[2, 7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val invalidPandasIds = Array(2, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "invalidPandasBcst = Broadcast(16)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val invalidPandasBcst = sc.broadcast(invalidPandasIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda(1,11000,giant,false,[D@633c0538)\n",
      "Panda(3,13000,small,true,[D@51090be8)\n",
      "Panda(4,13000,medium,false,[D@268e9067)\n",
      "Panda(5,18000,medium,true,[D@20ca715f)\n",
      "Panda(6,18000,giant,true,[D@30125e24)\n"
     ]
    }
   ],
   "source": [
    "pandasRDD.filter(panda => !invalidPandasBcst.value.contains(panda.id)).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasRDD.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda(6,18000,giant,true,[D@2819dda3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object LazyPrng\n",
       "bcastprng = Broadcast(18)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(18)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object LazyPrng extends Serializable{\n",
    "    \n",
    "    import java.util.Random\n",
    "    \n",
    "    @transient lazy val r = new Random()\n",
    "}\n",
    "\n",
    "val bcastprng = sc.broadcast(LazyPrng)\n",
    "pandasRDD.filter(x => bcastprng.value.r.nextInt(3) == 0).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccuFuzzyNess: DoubleAccumulator(id: 450, name: Some(fuzzyNess), value: 3.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accFuzzyNess = DoubleAccumulator(id: 450, name: Some(fuzzyNess), value: 3.0)\n",
       "transformed = MapPartitionsRDD[21] at map at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[21] at map at <console>:32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accFuzzyNess = sc.doubleAccumulator(\"fuzzyNess\")\n",
    "val transformed = pandasRDD.map(panda => {\n",
    "    accFuzzyNess.add(panda.attributes(0))\n",
    "    (panda.id, panda.zip)\n",
    "})\n",
    "transformed.count()\n",
    "println(\"AccuFuzzyNess: \" + accFuzzyNess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MaxDoubleAccumulator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.util.AccumulatorV2\n",
    "class MaxDoubleAccumulator extends AccumulatorV2[Double, Option[Double]] {\n",
    "    \n",
    "    var currentVal: Option[Double] = None\n",
    "    override def isZero = currentVal.isEmpty\n",
    "    \n",
    "    override def reset() = {\n",
    "        currentVal = None\n",
    "    }\n",
    "    \n",
    "    def copy() = {\n",
    "        \n",
    "        val newCopy = new MaxDoubleAccumulator()\n",
    "        newCopy.currentVal = currentVal\n",
    "        newCopy\n",
    "    }\n",
    "    \n",
    "    override def copyAndReset() = {\n",
    "        \n",
    "        new MaxDoubleAccumulator()\n",
    "        \n",
    "    }\n",
    "    \n",
    "    override def add(value: Double) = {\n",
    "        \n",
    "        // currentVal = Some(currentVal.map(acc => Math.max(acc, value)).getOrElse(value))\n",
    "        currentVal = Some(5.0)\n",
    "        \n",
    "    }\n",
    "    \n",
    "    override def merge(other: AccumulatorV2[Double, Option[Double]]) = {\n",
    "        other match {\n",
    "            \n",
    "            case otherFuzzy: MaxDoubleAccumulator => otherFuzzy.currentVal.foreach(value => add(value))\n",
    "            case _ => throw new Exception(\"Unexpected merge with unsopported type \" + other)\n",
    "            \n",
    "        }\n",
    "        \n",
    "        \n",
    "    }\n",
    "    \n",
    "    // override def value = currentVal\n",
    "    \n",
    "    override def value = currentVal\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc = MaxDoubleAccumulator(id: 604, name: Some(My accumulator), value: None)\n",
       "transformed = MapPartitionsRDD[33] at map at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MaxDoubleAccumulator(id: 604, name: Some(My accumulator), value: None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val acc = new MaxDoubleAccumulator()\n",
    "sc.register(acc, \"My accumulator\")\n",
    "val transformed = pandasRDD.repartition(1).map(x => {acc.add(x.attributes(0).toDouble); (x.id, x.zip)})\n",
    "transformed.count()\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,11000), (2,11000), (3,13000), (4,13000), (5,18000), (6,18000), (7,18000)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
