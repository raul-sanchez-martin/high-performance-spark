{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Effective Transformations\n",
    "\n",
    "In this chapter, we will explore advance features of RDDs in order to perform effective transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Object Creation\n",
    "\n",
    "Minimizing the number of objects that our program creates is a great way to optimize our calculations. We are going to visit some examples which display the same functionalities but with a different number of objects created during its execution. In particular, we are going to consider a intial data where we have the report cards of different instructors for different pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recordCars = ParallelCollectionRDD[0] at parallelize at <console>:27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at <console>:27"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val recordCars = sc.parallelize(Array((\"instructor1\", \"this is a happy panda\"),\n",
    "                                      (\"instructor1\", \"this is a very happy panda\"),\n",
    "                                      (\"instructor2\", \"good\"),\n",
    "                                      (\"instructor2\", \"happy\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over that data, we want to calculate, for each instructor: the longest woard, the mentions of happy and the average words of their reporst. For doing so, we are going to use different Aggregator classes, in which the number of objects created are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Generating new objects without reusing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class ReportCardMetrics\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// case class ReportCardMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MetricsCalculator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// class MetricsCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(instructor1,ReportCardMetrics(5,2,5.5)), (instructor2,ReportCardMetrics(5,1,1.0))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// recordCars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: With object reusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MetricsCalculatorReuseObjects\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// class MetricsCalculatorReuseObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(instructor1,ReportCardMetrics(5,2,5.5)), (instructor2,ReportCardMetrics(5,1,1.0))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// recordCars.aggregateByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3: Using Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MetricsCalculatorArrays\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// class MetricsCalculatorArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:31: error: value seqenceOp is not a member of Array[Int]\n",
       "                                seqOp = ((reportCardMetrics, reportCardText) => reportCardMetrics.seqenceOp(reportCardText)),\n",
       "                                                                                                  ^\n",
       "<console>:32: error: value compOp is not a member of Array[Int]\n",
       "                                combOp = ((x, y) => x.compOp(y))).map(x => (x._1, x._2.toReportCardMetrics)).collect()\n",
       "                                                      ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// recordCars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Operations\n",
    "\n",
    "In this section, we include some examples of set operations that can be done in Spark highighting some peculiarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substract example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddA = ParallelCollectionRDD[5] at parallelize at <console>:27\n",
       "rddB = ParallelCollectionRDD[6] at parallelize at <console>:28\n",
       "subtraction = MapPartitionsRDD[10] at subtract at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at subtract at <console>:29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddA = sc.parallelize(Array(1,2,3,4,4,4,4))\n",
    "val rddB = sc.parallelize(Array(3,4))\n",
    "// val subtraction = rddA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtraction.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(subtraction.count() < rddA.count() - rddB.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersection example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intersection = MapPartitionsRDD[16] at intersection at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val intersection = rddA\n",
    "intersection.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "union = UnionRDD[17] at union at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 4, 4, 4, 3, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val union = rddA.union(rddB)\n",
    "union.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(!rddA.collect().sorted.sameElements(union.collect().sorted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Setup Overhead\n",
    "\n",
    "In this section, we will see some aspects of Spark related Spark set-up and configuration per-partition. We will see some examples with Broadcast Variables and Accumulators (both are Shared Variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Panda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// case class Panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandasRDD = ParallelCollectionRDD[18] at parallelize at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[18] at parallelize at <console>:29"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pandasRDD = sc.parallelize(Seq(Panda(1, 11000, \"giant\", false, Array(0.2, 0.8)),\n",
    "                                   Panda(2, 11000, \"small\", false, Array(0.3, 0.1)),\n",
    "                                   Panda(3, 13000, \"small\", true, Array(0.9, 0.7)),\n",
    "                                   Panda(4, 13000, \"medium\", false, Array(0.5, 0.4)),\n",
    "                                   Panda(5, 18000, \"medium\", true, Array(0.7, 0.1)),\n",
    "                                   Panda(6, 18000, \"giant\", true, Array(0.1, 0.7)),\n",
    "                                   Panda(7, 18000, \"small\", true, Array(0.3, 0.9))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "invalidPandasIds = Array(2, 7)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[2, 7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val invalidPandasIds = Array(2, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "invalidPandasBcst = Broadcast(16)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val invalidPandasBcst ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda(1,11000,giant,false,[D@40243752)\n",
      "Panda(3,13000,small,true,[D@3484e074)\n",
      "Panda(4,13000,medium,false,[D@5f1bc7b8)\n",
      "Panda(5,18000,medium,true,[D@e68f765)\n",
      "Panda(6,18000,giant,true,[D@2b5e21d5)\n"
     ]
    }
   ],
   "source": [
    "// pandasRDD.filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasRDD.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda(4,13000,medium,false,[D@5594ea0)\n",
      "Panda(7,18000,small,true,[D@6fa70d55)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object LazyPrng\n",
       "bcastprng = Broadcast(18)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(18)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// object LazyPrng extends Serializable{\n",
    "    \n",
    "    \n",
    "// }\n",
    "\n",
    "// val bcastprng = sc\n",
    "// pandasRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators\n",
    "\n",
    "Built-in accumulator example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccuFuzzyNess: DoubleAccumulator(id: 450, name: Some(fuzzyNess), value: 3.0000000000000004)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accFuzzyNess = DoubleAccumulator(id: 450, name: Some(fuzzyNess), value: 3.0000000000000004)\n",
       "transformed = MapPartitionsRDD[21] at map at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[21] at map at <console>:32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accFuzzyNess = \n",
    "val transformed = \n",
    "transformed.count()\n",
    "println(\"AccuFuzzyNess: \" + accFuzzyNess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom accumulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MaxDoubleAccumulator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// import org.apache.spark.util.AccumulatorV2\n",
    "// class MaxDoubleAccumulator extends AccumulatorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc = MaxDoubleAccumulator(id: 476, name: Some(My accumulator), value: Some(0.9))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "transformed: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MaxDoubleAccumulator(id: 476, name: Some(My accumulator), value: Some(0.9))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val acc = new MaxDoubleAccumulator()\n",
    "sc.register(acc, \"My accumulator\")\n",
    "val transformed = pandasRDD.repartition(1).collect().foreach(x => {acc.add(x.attributes(0).toDouble); (x.id, x.zip)})\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing RDDs\n",
    "\n",
    "One good way to optimize Spark calculations is to reuse RDDs when needed. In this section, we will see come examples where this approach could be a good option.\n",
    "\n",
    "### Cases of Reuse\n",
    "\n",
    "#### Iterative Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validationSet = ParallelCollectionRDD[26] at parallelize at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[26] at parallelize at <console>:29"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val validationSet: RDD[(Double, Int)] = sc.parallelize(Array((2.0, 1), (5.0, 2), (3.0, 2), (7.0, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testSet = Array(MapPartitionsRDD[27] at mapValues at <console>:33, MapPartitionsRDD[28] at mapValues at <console>:34, ParallelCollectionRDD[26] at parallelize at <console>:29)\n",
       "errors = Array(1.0, 2.0, 0.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 0.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math\n",
    "\n",
    "val testSet: Array[RDD[(Double, Int)]] = Array(validationSet.mapValues(_ + 1),\n",
    "                                               validationSet.mapValues(_ + 2),\n",
    "                                               validationSet)\n",
    "validationSet.persist()\n",
    "val errors = testSet.map(rdd => {\n",
    "    val errorCount = rdd.join(validationSet).values.map(x => (math.pow(x._1 - x._2, 2), 1))\n",
    "    .reduce((x, y) => (x._1 + y._1, x._2 + y._2))\n",
    "    \n",
    "    math.pow(errorCount._1.toDouble/errorCount._2, 0.5)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Actions on the Same RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddA = ParallelCollectionRDD[44] at parallelize at <console>:32\n",
       "sorted = ShuffledRDD[47] at sortByKey at <console>:33\n",
       "count = 4\n",
       "sample = 2.0\n",
       "sampled = Array((1,a), (2,b))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(1,a), (2,b)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddA: RDD[(Int, String)] = sc.parallelize(Array((1, \"a\"), (4, \"d\"), (3, \"c\"), (2, \"b\")))\n",
    "val sorted = rddA.sortByKey()\n",
    "// sorted\n",
    "val count = sorted.count() // Action 1 on sorted\n",
    "val sample = count/2.0\n",
    "val sampled = sorted.take(sample.toInt) // Action 2 on sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Reuse: Cache, Persist, Checkpoint, Suffle Files\n",
    "\n",
    "`Checkpoint example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddA = ParallelCollectionRDD[48] at parallelize at <console>:35\n",
       "sorted = ShuffledRDD[51] at sortByKey at <console>:36\n",
       "count = 4\n",
       "sample = 2.0\n",
       "sampled = Array((1,a), (2,b))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(1,a), (2,b)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddA: RDD[(Int, String)] = sc.parallelize(Array((1, \"a\"), (4, \"d\"), (3, \"c\"), (2, \"b\")))\n",
    "val sorted = rddA.sortByKey()\n",
    "// sc\n",
    "// sorted\n",
    "val count = sorted.count() // Action 1 on sorted\n",
    "val sample = count/2.0\n",
    "val sampled = sorted.take(sample.toInt) // Action 2 on sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
