{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Testing and Validation\n",
    "\n",
    "As in any other programming field, testing and validation are essential in Spark data processing. Here we will see some exaples of how to perform that.\n",
    "\n",
    "## General Spark Unit Testing\n",
    "\n",
    "### Regular Spark Jobs (testing with RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Panda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Panda(Happiness: Double, Niceness: Double, Softness: Double, \n",
    "                 Sweetness: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [Happiness: double, Niceness: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Happiness: double, Niceness: double ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on DataFrame approx equality failed\n",
      "Test on DataFrame approx equality failed\n"
     ]
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq(Panda(15.0, 0.25, 2467.0, 0.0),\n",
    "                                   Panda(2.0, 1000, 35.4, 0.0),\n",
    "                                   Panda(10.0, 2.0, 50.0, 0.0),\n",
    "                                   Panda(3.0, 8.5, 0.2, 98.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedResults = Map(1 -> Set(3.0, 15.0), 2 -> Set(2.0, 1000.0), 3 -> Set(35.4, 2467.0), 4 -> Set(0.0, 98.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(1 -> Set(3.0, 15.0), 2 -> Set(2.0, 1000.0), 3 -> Set(35.4, 2467.0), 4 -> Set(0.0, 98.0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val expectedResults = Map(1 -> Set(3.0, 15.0), 2 -> Set(2.0, 1000.0), 3 -> Set(35.4, 2467.0), 4 -> Set(0.0, 98.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rankIndexs = Array(2, 4)\n",
       "resultV0 = Map(1 -> Set(3.0, 15.0), 2 -> Set(2.0, 1000.0), 3 -> Set(35.4, 2467.0), 4 -> Set(0.0, 98.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(1 -> Set(3.0, 15.0), 2 -> Set(2.0, 1000.0), 3 -> Set(35.4, 2467.0), 4 -> Set(0.0, 98.0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rankIndexs = Array(2, 4)\n",
    "var resultV0 = Map[Int, Iterable[Double]]()\n",
    "\n",
    "for (idx <- 1 to df.schema.length) {\n",
    "    \n",
    "    val colData = df.rdd.map(row => row.getDouble(idx - 1))\n",
    "    val sortedData = colData.sortBy(x => x).zipWithIndex()\n",
    "    val ranksOnly = sortedData.filter(x => rankIndexs.contains(x._2 + 1)).map(_._1)\n",
    "    \n",
    "    resultV0 += (idx -> ranksOnly.collect().toSet)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(expectedResults == resultV0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowLength = 4\n",
       "pairRDD = MapPartitionsRDD[40] at flatMap at <console>:34\n",
       "resultV1 = Map(2 -> Set(2.0, 1000.0), 4 -> Set(0.0, 98.0), 1 -> Set(3.0, 15.0), 3 -> Set(35.4, 2467.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(2 -> Set(2.0, 1000.0), 4 -> Set(0.0, 98.0), 1 -> Set(3.0, 15.0), 3 -> Set(35.4, 2467.0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rowLength = df.schema.length\n",
    "val pairRDD = df.rdd.flatMap(row => Range(0, rowLength).map(idx => (idx, row.getDouble(idx))))\n",
    "val resultV1 = pairRDD.groupByKey().map(x => (x._1, x._2.toArray.sorted.zipWithIndex\n",
    "                                            .filter(y => rankIndexs.contains(y._2 + 1))\n",
    "                                              .map(x => (x._1)))).map(x => (x._1+1, x._2.toSet)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(expectedResults == resultV1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowLength = 4\n",
       "pairRDD = MapPartitionsRDD[44] at flatMap at <console>:37\n",
       "badResult = Map(2 -> Set(3.0, 15.0), 5 -> Set(0.0, 98.0), 4 -> Set(35.4, 2467.0), 3 -> Set(2.0, 1000.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(2 -> Set(3.0, 15.0), 5 -> Set(0.0, 98.0), 4 -> Set(35.4, 2467.0), 3 -> Set(2.0, 1000.0))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rowLength = df.schema.length\n",
    "val pairRDD = df.rdd.flatMap(row => Range(0, rowLength).map(idx => (idx, row.getDouble(idx))))\n",
    "val badResult = pairRDD.groupByKey().map(x => (x._1, x._2.toArray.sorted.zipWithIndex\n",
    "                                               .filter(y => rankIndexs.contains(y._2 + 1))\n",
    "                                               .map(x => (x._1)))).map(x => (x._1+2, x._2.toSet)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.AssertionError\n",
       "Message: assertion failed\n",
       "StackTrace:   at scala.Predef$.assert(Predef.scala:156)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert(expectedResults == badResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking RDDs\n",
    "\n",
    "### Testing DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n",
       "defined class PersonSimplifed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Person(id: Int, name:String, age:Int, balance:Double)\n",
    "case class PersonSimplifed(id: Int, name:String, age:Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedDf = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val expectedDf = spark.createDataFrame(Seq(Person(1, \"John\", 23, 145.2),\n",
    "                                           Person(2, \"Maria\", 65, 248.3),\n",
    "                                           Person(3, \"Peter\", 39, 458.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "goodResultDf = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val goodResultDf = spark.createDataFrame(Seq(Person(1, \"John\", 23, 145.2),\n",
    "                                             Person(2, \"Maria\", 65, 248.3),\n",
    "                                             Person(3, \"Peter\", 39, 458.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "badResultDf1 = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val badResultDf1 = spark.createDataFrame(Seq(Person(1, \"John\", 23, 145.2),\n",
    "                                             Person(2, \"Maria\", 65, 248.3),\n",
    "                                             Person(3, \"Peter\", 39, 45.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "badResultDf2 = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val badResultDf2 = spark.createDataFrame(Seq(Person(1, \"John\", 23, 145.2),\n",
    "                                             Person(2, \"Maria\", 37, 248.3),\n",
    "                                             Person(3, \"Peter\", 39, 458.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "badResultDf3 = [id: int, name: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 1 more field]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val badResultDf3 = spark.createDataFrame(Seq(PersonSimplifed(1, \"John\", 23),\n",
    "                                             PersonSimplifed(2, \"Maria\", 65),\n",
    "                                             PersonSimplifed(3, \"Peter\", 39)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "badResultDf4 = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val badResultDf4 = spark.createDataFrame(Seq(Person(1, \"John\", 23, 145.2),\n",
    "                                             Person(2, \"Maria\", 65, 248.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "checkEqualityItems: (a: Any, b: Any)Boolean\n",
       "checkApproximateEqualityItems: (a: Double, b: Double, tol: Double)Boolean\n",
       "assertDataFrameApproximateEquals: (expectedDf: org.apache.spark.sql.DataFrame, resultDf: org.apache.spark.sql.DataFrame, tol: Double)Boolean\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "Check complete equality between two objects\n",
    "\n",
    "@param a: first item to compare\n",
    "@param b: second item to compare\n",
    "@return: boolean indicating if the two objects are equal or not\n",
    "**/\n",
    "\n",
    "def checkEqualityItems(a: Any, b: Any): Boolean = {\n",
    "    try {\n",
    "        assert(a==b)\n",
    "        true\n",
    "            \n",
    "    } catch {\n",
    "        case a: java.lang.AssertionError => {println(\"Test on DataFrame approx equality failed\"); false}\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "/**\n",
    "Checks approximate equality between two booleans. The approximation\n",
    "is set by a tolerance, given as an input\n",
    "\n",
    "@param a: first Double to compare\n",
    "@param b: second Double to compare\n",
    "@param tol: tolerance\n",
    "@return: boolean indicating if the two numbers are equal or not\n",
    "**/\n",
    "\n",
    "def checkApproximateEqualityItems(a: Double, b: Double, tol: Double): Boolean = {\n",
    "    try {\n",
    "        assert(b <= a + tol)\n",
    "        assert(b >= a - tol)\n",
    "        true\n",
    "            \n",
    "    } catch {\n",
    "        case a: java.lang.AssertionError => {println(\"Test on DataFrame approx equality failed\"); false}\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "/**\n",
    "Check if two DataFrames are equal, considering a tolerance for Double items of the DataFrames\n",
    "\n",
    "@param a: first DataFrame to compare\n",
    "@param b: second DataFrame to compare\n",
    "@param tol: tolerance\n",
    "@return: boolean indicating if the two DataFrames are equal or not\n",
    "**/\n",
    "\n",
    "def assertDataFrameApproximateEquals(expectedDf: DataFrame, resultDf: DataFrame, tol: Double): Boolean = {\n",
    "    \n",
    "    try {\n",
    "        \n",
    "        // Check Schema\n",
    "        assert(expectedDf.schema == resultDf.schema)\n",
    "        \n",
    "        // Check number of rows\n",
    "        assert(expectedDf.rdd.count() == resultDf.rdd.count())\n",
    "        \n",
    "        // Check row content\n",
    "        val rowContentCheck = expectedDf.rdd.zip(resultDf.rdd).flatMap(x => x._1.toSeq.zip(x._2.toSeq)).map{\n",
    "    \n",
    "            case (a: Int, b: Int) => checkEqualityItems(a, b)\n",
    "            case (a: String, b: String) => checkEqualityItems(a, b)\n",
    "            case (a: Double, b: Double) => checkApproximateEqualityItems(a, b, tol)\n",
    "\n",
    "        }.filter(_ == false).collect().length\n",
    "        assert(rowContentCheck == 0)\n",
    "        \n",
    "        true\n",
    "        \n",
    "    }\n",
    "    \n",
    "    catch {\n",
    "        \n",
    "        case a: java.lang.AssertionError => {println(\"Test on DataFrame approx equality failed\"); false}\n",
    "    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertDataFrameApproximateEquals(expectedDf, goodResultDf, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on DataFrame approx equality failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertDataFrameApproximateEquals(expectedDf, badResultDf1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on DataFrame approx equality failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertDataFrameApproximateEquals(expectedDf, badResultDf2, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on DataFrame approx equality failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertDataFrameApproximateEquals(expectedDf, badResultDf3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on DataFrame approx equality failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertDataFrameApproximateEquals(expectedDf, badResultDf4, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Test Data\n",
    "\n",
    "### Generating Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.random.RandomRDDs\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class RawPanda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class RawPanda(id: Long, zip: String, pt: String,\n",
    "  happy: Boolean, attributes: Array[Double])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generateGoldilocks: (sc: org.apache.spark.SparkContext, rows: Long, numCols: Int)org.apache.spark.rdd.RDD[RawPanda]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generateGoldilocks(sc: SparkContext, rows: Long, numCols: Int):\n",
    "      RDD[RawPanda] = {\n",
    "    val zipRDD = RandomRDDs.exponentialRDD(sc, mean = 1000,  size = rows)\n",
    "      .map(_.toInt.toString)\n",
    "    val valuesRDD = RandomRDDs.normalVectorRDD(\n",
    "      sc, numRows = rows, numCols = numCols)\n",
    "    zipRDD.zip(valuesRDD).map{case (z, v) =>\n",
    "      RawPanda(1, z, \"giant\", v(0) > 0.5, v.toArray)\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "goldilockData = MapPartitionsRDD[84] at map at <console>:40\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[84] at map at <console>:40"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val goldilockData = generateGoldilocks(sc, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RawPanda(1,1611,giant,false,[D@2dde35d9), RawPanda(1,1453,giant,true,[D@68515268), RawPanda(1,319,giant,true,[D@8f3fd8a), RawPanda(1,14,giant,false,[D@65ecdfcc), RawPanda(1,2696,giant,false,[D@39c05e55)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldilockData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "valuesRDD = RandomVectorRDD[85] at RDD at RandomRDD.scala:64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RandomVectorRDD[85] at RDD at RandomRDD.scala:64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val valuesRDD = RandomRDDs.normalVectorRDD(\n",
    "      sc, numRows = 10, numCols = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = ParallelCollectionRDD[86] at parallelize at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[86] at parallelize at <console>:32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = sc.parallelize(\n",
    "  Seq((1, 'a'), (1, 'b'), (2, 'c'), (2, 'd'), (2, 'e'), (3, 'f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampledData = PartitionwiseSampledRDD[87] at sample at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PartitionwiseSampledRDD[87] at sample at <console>:34"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampledData = data.sample(withReplacement = false, fraction = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,b), (2,c), (2,e), (3,f)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fractions = Map(1 -> 0.5, 2 -> 0.5, 3 -> 0.3)\n",
       "approxSample = MapPartitionsRDD[88] at sampleByKey at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[88] at sampleByKey at <console>:38"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// specify the exact fraction desired from each key\n",
    "val fractions = Map(1 -> 0.5, 2 -> 0.5, 3 -> 0.3)\n",
    "\n",
    "// Get an approximate sample from each stratum\n",
    "val approxSample = data.sampleByKey(withReplacement = false, fractions = fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,a), (1,b)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approxSample.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property Checking with ScalaCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assertRDDEqualsWithOrder: [T](expected: org.apache.spark.rdd.RDD[T], result: org.apache.spark.rdd.RDD[T])(implicit evidence$1: scala.reflect.ClassTag[T])Unit\n",
       "compareRDDWithOrder: [T](expected: org.apache.spark.rdd.RDD[T], result: org.apache.spark.rdd.RDD[T])(implicit evidence$2: scala.reflect.ClassTag[T])Option[(Option[T], Option[T])]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.reflect.ClassTag\n",
    "\n",
    "\n",
    "/**\n",
    "Check if two RDDs are equal (with order)\n",
    "**/\n",
    "def assertRDDEqualsWithOrder[T: ClassTag](expected: RDD[T], result: RDD[T]): Unit = {\n",
    "    \n",
    "    assert(compareRDDWithOrder(expected, result).isEmpty)\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def compareRDDWithOrder[T: ClassTag](expected: RDD[T], result: RDD[T]): Option[(Option[T], Option[T])] = {\n",
    "    \n",
    "    val indexedExpected = expected.zipWithIndex.map{case(x, y) => (y, x)}\n",
    "    val indexedResult = result.zipWithIndex.map{case(x, y) => (y, x)}\n",
    "    \n",
    "    indexedExpected.cogroup(indexedResult).filter{case(_,(i1, i2)) => i1.isEmpty || i2.isEmpty || i1.head != i2.head}.\n",
    "            take(1).headOption.map{case(_, (i1, i2)) => (i1.headOption, i2.headOption)}.take(1).headOption\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1 = ParallelCollectionRDD[89] at parallelize at <console>:33\n",
       "rdd2 = ParallelCollectionRDD[90] at parallelize at <console>:34\n",
       "rdd3 = ParallelCollectionRDD[91] at parallelize at <console>:35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[91] at parallelize at <console>:35"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(Array(1,5,8))\n",
    "val rdd2 = sc.parallelize(Array(8,5,3))\n",
    "val rdd3 = sc.parallelize(Array(1,5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.AssertionError\n",
       "Message: assertion failed\n",
       "StackTrace:   at scala.Predef$.assert(Predef.scala:156)\n",
       "  at assertRDDEqualsWithOrder(<console>:38)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertRDDEqualsWithOrder(rdd1, rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assertRDDEqualsWithOrder(rdd1, rdd3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class PerfListener\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.scheduler.{SparkListener, SparkListenerTaskEnd}\n",
    "// import org.apache.spark.status.api.v1.TaskMetrics\n",
    "import org.apache.spark.executor.TaskMetrics\n",
    "\n",
    "class PerfListener extends SparkListener {\n",
    "    var totalExecutiorRunTime = 0L\n",
    "    \n",
    "    override def onTaskEnd(taskEnd: SparkListenerTaskEnd) {\n",
    "        val info = taskEnd.taskInfo\n",
    "        val metrics = taskEnd.taskMetrics\n",
    "        updateMetricsForTask(metrics)\n",
    "        \n",
    "    }\n",
    "    \n",
    "    private def updateMetricsForTask(metrics: TaskMetrics): Unit = {\n",
    "        totalExecutiorRunTime += metrics.executorRunTime\n",
    "        \n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "listener = PerfListener@2b20be55\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "$line131.$read$$iw$$iw$PerfListener@2b20be55"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val listener = new PerfListener()\n",
    "sc.addSparkListener(listener)\n",
    "sc.parallelize(1 to 1000000).count()\n",
    "assert(listener.totalExecutiorRunTime < 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
