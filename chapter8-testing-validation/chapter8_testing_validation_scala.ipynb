{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Testing and Validation\n",
    "\n",
    "As in any other programming field, testing and validation are essential in Spark data processing. Here we will see some exaples of how to perform that.\n",
    "\n",
    "## General Spark Unit Testing\n",
    "\n",
    "### Regular Spark Jobs (testing with RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Panda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Panda(Happiness: Double, Niceness: Double, Softness: Double, \n",
    "                 Sweetness: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [Happiness: double, Niceness: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Happiness: double, Niceness: double ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on DataFrame approx equality failed\n",
      "Test on DataFrame approx equality failed\n"
     ]
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq(Panda(15.0, 0.25, 2467.0, 0.0),\n",
    "                                   Panda(2.0, 1000, 35.4, 0.0),\n",
    "                                   Panda(10.0, 2.0, 50.0, 0.0),\n",
    "                                   Panda(3.0, 8.5, 0.2, 98.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedResults = Map(1 -> Set(3.0, 15.0), 2 -> Set(2.0, 1000.0), 3 -> Set(35.4, 2467.0), 4 -> Set(0.0, 98.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(1 -> Set(3.0, 15.0), 2 -> Set(2.0, 1000.0), 3 -> Set(35.4, 2467.0), 4 -> Set(0.0, 98.0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val expectedResults = Map(1 -> Set(3.0, 15.0), 2 -> Set(2.0, 1000.0), 3 -> Set(35.4, 2467.0), 4 -> Set(0.0, 98.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rankIndexs = Array(2, 4)\n",
       "resultV0 = Map(1 -> Set(3.0, 15.0), 2 -> Set(2.0, 1000.0), 3 -> Set(35.4, 2467.0), 4 -> Set(0.0, 98.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(1 -> Set(3.0, 15.0), 2 -> Set(2.0, 1000.0), 3 -> Set(35.4, 2467.0), 4 -> Set(0.0, 98.0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rankIndexs = Array(2, 4)\n",
    "var resultV0 = Map[Int, Iterable[Double]]()\n",
    "\n",
    "for (idx <- 1 to df.schema.length) {\n",
    "    \n",
    "    val colData = df.rdd.map(row => row.getDouble(idx - 1))\n",
    "    val sortedData = colData.sortBy(x => x).zipWithIndex()\n",
    "    val ranksOnly = sortedData.filter(x => rankIndexs.contains(x._2 + 1)).map(_._1)\n",
    "    \n",
    "    resultV0 += (idx -> ranksOnly.collect().toSet)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(expectedResults == resultV0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowLength = 4\n",
       "pairRDD = MapPartitionsRDD[40] at flatMap at <console>:34\n",
       "resultV1 = Map(2 -> Set(2.0, 1000.0), 4 -> Set(0.0, 98.0), 1 -> Set(3.0, 15.0), 3 -> Set(35.4, 2467.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(2 -> Set(2.0, 1000.0), 4 -> Set(0.0, 98.0), 1 -> Set(3.0, 15.0), 3 -> Set(35.4, 2467.0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rowLength = df.schema.length\n",
    "val pairRDD = df.rdd.flatMap(row => Range(0, rowLength).map(idx => (idx, row.getDouble(idx))))\n",
    "val resultV1 = pairRDD.groupByKey().map(x => (x._1, x._2.toArray.sorted.zipWithIndex\n",
    "                                            .filter(y => rankIndexs.contains(y._2 + 1))\n",
    "                                              .map(x => (x._1)))).map(x => (x._1+1, x._2.toSet)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(expectedResults == resultV1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowLength = 4\n",
       "pairRDD = MapPartitionsRDD[44] at flatMap at <console>:37\n",
       "badResult = Map(2 -> Set(3.0, 15.0), 5 -> Set(0.0, 98.0), 4 -> Set(35.4, 2467.0), 3 -> Set(2.0, 1000.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(2 -> Set(3.0, 15.0), 5 -> Set(0.0, 98.0), 4 -> Set(35.4, 2467.0), 3 -> Set(2.0, 1000.0))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rowLength = df.schema.length\n",
    "val pairRDD = df.rdd.flatMap(row => Range(0, rowLength).map(idx => (idx, row.getDouble(idx))))\n",
    "val badResult = pairRDD.groupByKey().map(x => (x._1, x._2.toArray.sorted.zipWithIndex\n",
    "                                               .filter(y => rankIndexs.contains(y._2 + 1))\n",
    "                                               .map(x => (x._1)))).map(x => (x._1+2, x._2.toSet)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.AssertionError\n",
       "Message: assertion failed\n",
       "StackTrace:   at scala.Predef$.assert(Predef.scala:156)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert(expectedResults == badResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking RDDs\n",
    "\n",
    "### Testing DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n",
       "defined class PersonSimplifed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Person(id: Int, name:String, age:Int, balance:Double)\n",
    "case class PersonSimplifed(id: Int, name:String, age:Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedDf = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val expectedDf = spark.createDataFrame(Seq(Person(1, \"John\", 23, 145.2),\n",
    "                                           Person(2, \"Maria\", 65, 248.3),\n",
    "                                           Person(3, \"Peter\", 39, 458.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "goodResultDf = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val goodResultDf = spark.createDataFrame(Seq(Person(1, \"John\", 23, 145.2),\n",
    "                                             Person(2, \"Maria\", 65, 248.3),\n",
    "                                             Person(3, \"Peter\", 39, 458.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "badResultDf1 = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val badResultDf1 = spark.createDataFrame(Seq(Person(1, \"John\", 23, 145.2),\n",
    "                                             Person(2, \"Maria\", 65, 248.3),\n",
    "                                             Person(3, \"Peter\", 39, 45.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "badResultDf2 = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val badResultDf2 = spark.createDataFrame(Seq(Person(1, \"John\", 23, 145.2),\n",
    "                                             Person(2, \"Maria\", 37, 248.3),\n",
    "                                             Person(3, \"Peter\", 39, 458.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "badResultDf3 = [id: int, name: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 1 more field]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val badResultDf3 = spark.createDataFrame(Seq(PersonSimplifed(1, \"John\", 23),\n",
    "                                             PersonSimplifed(2, \"Maria\", 65),\n",
    "                                             PersonSimplifed(3, \"Peter\", 39)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "badResultDf4 = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val badResultDf4 = spark.createDataFrame(Seq(Person(1, \"John\", 23, 145.2),\n",
    "                                             Person(2, \"Maria\", 65, 248.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "checkEqualityItems: (a: Any, b: Any)Boolean\n",
       "checkApproximateEqualityItems: (a: Double, b: Double, tol: Double)Boolean\n",
       "assertDataFrameApproximateEquals: (expectedDf: org.apache.spark.sql.DataFrame, resultDf: org.apache.spark.sql.DataFrame, tol: Double)Boolean\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "Check complete equality between two objects\n",
    "\n",
    "@param a: first item to compare\n",
    "@param b: second item to compare\n",
    "@return: boolean indicating if the two objects are equal or not\n",
    "**/\n",
    "\n",
    "def checkEqualityItems(a: Any, b: Any): Boolean = {\n",
    "    try {\n",
    "        assert(a==b)\n",
    "        true\n",
    "            \n",
    "    } catch {\n",
    "        case a: java.lang.AssertionError => {println(\"Test on DataFrame approx equality failed\"); false}\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "/**\n",
    "Checks approximate equality between two booleans. The approximation\n",
    "is set by a tolerance, given as an input\n",
    "\n",
    "@param a: first Double to compare\n",
    "@param b: second Double to compare\n",
    "@param tol: tolerance\n",
    "@return: boolean indicating if the two numbers are equal or not\n",
    "**/\n",
    "\n",
    "def checkApproximateEqualityItems(a: Double, b: Double, tol: Double): Boolean = {\n",
    "    try {\n",
    "        assert(b <= a + tol)\n",
    "        assert(b >= a - tol)\n",
    "        true\n",
    "            \n",
    "    } catch {\n",
    "        case a: java.lang.AssertionError => {println(\"Test on DataFrame approx equality failed\"); false}\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "/**\n",
    "Check if two DataFrames are equal, considering a tolerance for Double items of the DataFrames\n",
    "\n",
    "@param a: first DataFrame to compare\n",
    "@param b: second DataFrame to compare\n",
    "@param tol: tolerance\n",
    "@return: boolean indicating if the two DataFrames are equal or not\n",
    "**/\n",
    "\n",
    "def assertDataFrameApproximateEquals(expectedDf: DataFrame, resultDf: DataFrame, tol: Double): Boolean = {\n",
    "    \n",
    "    try {\n",
    "        \n",
    "        // Check Schema\n",
    "        assert(expectedDf.schema == resultDf.schema)\n",
    "        \n",
    "        // Check number of rows\n",
    "        assert(expectedDf.rdd.count() == resultDf.rdd.count())\n",
    "        \n",
    "        // Check row content\n",
    "        val rowContentCheck = expectedDf.rdd.zip(resultDf.rdd).flatMap(x => x._1.toSeq.zip(x._2.toSeq)).map{\n",
    "    \n",
    "            case (a: Int, b: Int) => checkEqualityItems(a, b)\n",
    "            case (a: String, b: String) => checkEqualityItems(a, b)\n",
    "            case (a: Double, b: Double) => checkApproximateEqualityItems(a, b, tol)\n",
    "\n",
    "        }.filter(_ == false).collect().length\n",
    "        assert(rowContentCheck == 0)\n",
    "        \n",
    "        true\n",
    "        \n",
    "    }\n",
    "    \n",
    "    catch {\n",
    "        \n",
    "        case a: java.lang.AssertionError => {println(\"Test on DataFrame approx equality failed\"); false}\n",
    "    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertDataFrameApproximateEquals(expectedDf, goodResultDf, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on DataFrame approx equality failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertDataFrameApproximateEquals(expectedDf, badResultDf1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on DataFrame approx equality failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertDataFrameApproximateEquals(expectedDf, badResultDf2, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on DataFrame approx equality failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertDataFrameApproximateEquals(expectedDf, badResultDf3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on DataFrame approx equality failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertDataFrameApproximateEquals(expectedDf, badResultDf4, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
