{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Working with Key/Value Data\n",
    "\n",
    "In this notebook, we will see some advanced concepts when performing data computations on key/value RDDs. We will solve in different ways the Goldilocks Example, focusing on performance considerations.\n",
    "\n",
    "## The Goldlocks Example\n",
    "\n",
    "In this example, we have data representing some metrics of several pandas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Panda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Panda(Happiness: Double, Niceness: Double, Softness: Double, \n",
    "                 Sweetness: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [Happiness: double, Niceness: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Happiness: double, Niceness: double ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq(Panda(15.0, 0.25, 2467.0, 0.0),\n",
    "                                   Panda(2.0, 1000, 35.4, 0.0),\n",
    "                                   Panda(10.0, 2.0, 50.0, 0.0),\n",
    "                                   Panda(3.0, 8.5, 0.2, 98.0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------+---------+\n",
      "|Happiness|Niceness|Softness|Sweetness|\n",
      "+---------+--------+--------+---------+\n",
      "|     15.0|    0.25|  2467.0|      0.0|\n",
      "|      2.0|  1000.0|    35.4|      0.0|\n",
      "|     10.0|     2.0|    50.0|      0.0|\n",
      "|      3.0|     8.5|     0.2|     98.0|\n",
      "+---------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to design an algorithm that allow us to input an arbitrary list of integers n1...nk and return the nth best element in each column. In particular, se are going to 2nd and 4th elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks Version 0: Iterative Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,WrappedArray(3.0, 15.0))\n",
      "(2,WrappedArray(2.0, 1000.0))\n",
      "(3,WrappedArray(35.4, 2467.0))\n",
      "(4,WrappedArray(0.0, 98.0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rankIndexs = Array(2, 4)\n",
       "resultV0 = Map(1 -> WrappedArray(3.0, 15.0), 2 -> WrappedArray(2.0, 1000.0), 3 -> WrappedArray(35.4, 2467.0), 4 -> WrappedArray(0.0, 98.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(1 -> WrappedArray(3.0, 15.0), 2 -> WrappedArray(2.0, 1000.0), 3 -> WrappedArray(35.4, 2467.0), 4 -> WrappedArray(0.0, 98.0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rankIndexs = Array(2, 4)\n",
    "var resultV0 = Map[Int, Iterable[Double]]()\n",
    "\n",
    "for (idx <- 1 to df.schema.length) {\n",
    "    \n",
    "    val colData = df.rdd.map(row => row.getDouble(idx - 1))\n",
    "    val sortedData = colData.sortBy(x => x).zipWithIndex()\n",
    "    val ranksOnly = sortedData.filter(x => rankIndexs.contains(x._2 + 1)).map(_._1)\n",
    "    \n",
    "    resultV0 += (idx -> ranksOnly.collect())\n",
    "    \n",
    "}\n",
    "\n",
    "resultV0.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks Version 1: GroupByKey Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rankIndexs = Array(2, 4)\n",
       "rowLength = 4\n",
       "pairRDD = MapPartitionsRDD[40] at flatMap at <console>:35\n",
       "resultV1 = Map(2 -> Array(35.4, 2467.0), 1 -> Array(2.0, 1000.0), 3 -> Array(0.0, 98.0), 0 -> Array(3.0, 15.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(2 -> [D@33416fef, 1 -> [D@234553d2, 3 -> [D@4a72e0a9, 0 -> [D@7bd123f1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rankIndexs = Array(2, 4)\n",
    "val rowLength = df.schema.length\n",
    "val pairRDD = df.rdd.flatMap(row => Range(0, rowLength).map(idx => (idx, row.getDouble(idx))))\n",
    "val resultV1 = pairRDD.groupByKey().map(x => (x._1, x._2.toArray.sorted.zipWithIndex\n",
    "                                             .filter(y => rankIndexs.contains(y._2 + 1)).map(_._1))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-> 3.0, 15.0\n",
      "1-> 2.0, 1000.0\n",
      "2-> 35.4, 2467.0\n",
      "3-> 0.0, 98.0\n"
     ]
    }
   ],
   "source": [
    "resultV1.toSeq.sortBy(_._1).foreach({case (key, value) => println(key + \"-> \" + value.mkString(\", \"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioners and Key/Value Data\n",
    "\n",
    "### Preserving Partitioning Information Across Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partitioner = org.apache.spark.HashPartitioner@4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.HashPartitioner@4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val partitioner = new HashPartitioner(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddA = ShuffledRDD[61] at partitionBy at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[61] at partitionBy at <console>:32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddA = sc.parallelize(Array(1,2,3,4)).map(x => (x, x*1.5)).partitionBy(partitioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(org.apache.spark.HashPartitioner@4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddA.partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddA.map(x => x).partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(org.apache.spark.HashPartitioner@4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddA.mapValues(x => x).partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-located RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a = MapPartitionsRDD[65] at map at <console>:36\n",
       "b = MapPartitionsRDD[67] at map at <console>:37\n",
       "rddA = ShuffledRDD[68] at partitionBy at <console>:39\n",
       "rddB = ShuffledRDD[69] at partitionBy at <console>:41\n",
       "rddC = MapPartitionsRDD[71] at cogroup at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = sc.parallelize(Array(1,2,3,4)).map(x => (x, x*1.5))\n",
    "val b = sc.parallelize(Array(2,4,7,9)).map(x => (x, x*1.5))\n",
    "\n",
    "val rddA = a.partitionBy(partitioner)\n",
    "rddA.cache()\n",
    "val rddB = b.partitionBy(partitioner)\n",
    "rddB.cache()\n",
    "val rddC = a.cogroup(b)\n",
    "rddC.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs Co-partitioned but not Co-Located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a = MapPartitionsRDD[73] at map at <console>:38\n",
       "b = MapPartitionsRDD[75] at map at <console>:39\n",
       "rddA = ShuffledRDD[76] at partitionBy at <console>:41\n",
       "rddB = ShuffledRDD[77] at partitionBy at <console>:43\n",
       "rddC = MapPartitionsRDD[79] at cogroup at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = sc.parallelize(Array(1,2,3,4)).map(x => (x, x*1.5))\n",
    "val b = sc.parallelize(Array(2,4,7,9)).map(x => (x, x*1.5))\n",
    "\n",
    "val rddA = a.partitionBy(partitioner)\n",
    "rddA.cache()\n",
    "val rddB = b.partitionBy(partitioner)\n",
    "rddB.cache()\n",
    "val rddC = a.cogroup(b)\n",
    "rddA.count()\n",
    "rddB.count()\n",
    "rddC.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks Version 2: Secondary Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [Happiness: double, Niceness: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Happiness: double, Niceness: double ... 2 more fields]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq(Panda(15.0, 0.25, 2467.0, 0.0),\n",
    "                                   Panda(2.0, 1000, 35.4, 0.0),\n",
    "                                   Panda(10.0, 2.0, 50.0, 0.0),\n",
    "                                   Panda(3.0, 8.5, 0.2, 98.0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class ColumnIndexPartition\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.Partitioner\n",
    "import scala.math\n",
    "\n",
    "class ColumnIndexPartition(override val numPartitions: Int) extends Partitioner {\n",
    "    \n",
    "    require(numPartitions >=0, s\"Number of partitions $numPartitions cannot be negative\")\n",
    "    \n",
    "    override def getPartition(key: Any) = {\n",
    "        \n",
    "        val k = key.asInstanceOf[(Int, Double)]\n",
    "        \n",
    "        math.abs(k._1) % numPartitions\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairRDD = MapPartitionsRDD[95] at map at <console>:45\n",
       "partitioner = ColumnIndexPartition@35214ad6\n",
       "sorted = ShuffledRDD[96] at repartitionAndSortWithinPartitions at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[96] at repartitionAndSortWithinPartitions at <console>:47"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairRDD = df.rdd.flatMap(row => Range(0, rowLength).map(idx => (idx, row.getDouble(idx)))).map((_, 1))\n",
    "val partitioner = new ColumnIndexPartition(4)\n",
    "val sorted = pairRDD.repartitionAndSortWithinPartitions(partitioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0,2.0),1), ((0,3.0),1), ((0,10.0),1), ((0,15.0),1), ((1,0.25),1), ((1,2.0),1), ((1,8.5),1), ((1,1000.0),1), ((2,0.2),1), ((2,35.4),1), ((2,50.0),1), ((2,2467.0),1), ((3,0.0),1), ((3,0.0),1), ((3,0.0),1), ((3,98.0),1)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "targetRanks = Array(2, 4)\n",
       "filterForTargetIndex = MapPartitionsRDD[99] at mapPartitions at <console>:50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[99] at mapPartitions at <console>:50"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val targetRanks = Array(2,4)\n",
    "val filterForTargetIndex: RDD[(Int, Double)] = sorted.mapPartitions(iter =>{\n",
    "    \n",
    "    var currentColumnIndex = -1\n",
    "    var runningTotal = 0\n",
    "    iter.filter({case(((colIndex, value), _)) =>\n",
    "        if(((colIndex != currentColumnIndex))) {\n",
    "            currentColumnIndex = colIndex\n",
    "            runningTotal = 1\n",
    "            \n",
    "        } else {\n",
    "            \n",
    "            runningTotal += 1\n",
    "        }\n",
    "        \n",
    "        targetRanks.contains(runningTotal)\n",
    "    })\n",
    "}.map(_._1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultV2 = Map()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var resultV2 = Map[Int, Iterable[Double]]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultV2 += 1 -> Seq(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:38: warning: match may not be exhaustive.\n",
       "It would fail on the following input: None\n",
       "       resultV2.get(1) match {\n",
       "                   ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultV2.get(1) match {\n",
    "    case Some(value) => {resultV2 += 1 -> {value ++ Seq(2.0)}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(List(1.0, 2.0))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultV2.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List(1.0, 2.0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1.0) ++ Seq(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,3.0), (0,15.0), (1,2.0), (1,1000.0), (2,35.4), (2,2467.0), (3,0.0), (3,98.0)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterForTargetIndex.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultV2 = Map(0 -> List(3.0, 15.0), 1 -> List(2.0, 1000.0), 2 -> List(35.4, 2467.0), 3 -> List(0.0, 98.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(0 -> List(3.0, 15.0), 1 -> List(2.0, 1000.0), 2 -> List(35.4, 2467.0), 3 -> List(0.0, 98.0))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var resultV2 = Map[Int, Iterable[Double]]()\n",
    "filterForTargetIndex.collect().foreach(x => {\n",
    "    resultV2.get(x._1) match {\n",
    "        case Some(value) => {resultV2 += x._1 -> {value ++ Seq(x._2.toDouble)}}\n",
    "        case None => {resultV2 += x._1 -> Seq(x._2.toDouble)}  \n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-> 3.0, 15.0\n",
      "1-> 2.0, 1000.0\n",
      "2-> 35.4, 2467.0\n",
      "3-> 0.0, 98.0\n"
     ]
    }
   ],
   "source": [
    "resultV2.toSeq.sortBy(_._1).foreach({case (key, value) => println(key + \"-> \" + value.mkString(\", \"))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
