{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Working with Key/Value Data\n",
    "\n",
    "In this notebook, we will see some advanced concepts when performing data computations on key/value RDDs. We will solve in different ways the Goldilocks Example, focusing on performance considerations.\n",
    "\n",
    "## The Goldlocks Example\n",
    "\n",
    "In this example, we have data representing some metrics of several pandas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Panda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// case class Panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [Happiness: double, Niceness: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Happiness: double, Niceness: double ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq(Panda(15.0, 0.25, 2467.0, 0.0),\n",
    "                                   Panda(2.0, 1000, 35.4, 0.0),\n",
    "                                   Panda(10.0, 2.0, 50.0, 0.0),\n",
    "                                   Panda(3.0, 8.5, 0.2, 98.0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------+---------+\n",
      "|Happiness|Niceness|Softness|Sweetness|\n",
      "+---------+--------+--------+---------+\n",
      "|     15.0|    0.25|  2467.0|      0.0|\n",
      "|      2.0|  1000.0|    35.4|      0.0|\n",
      "|     10.0|     2.0|    50.0|      0.0|\n",
      "|      3.0|     8.5|     0.2|     98.0|\n",
      "+---------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to design an algorithm that allow us to input an arbitrary list of integers n1...nk and return the nth best element in each column. In particular, se are going to 2nd and 4th elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks Version 0: Iterative Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,WrappedArray(3.0, 15.0))\n",
      "(2,WrappedArray(2.0, 1000.0))\n",
      "(3,WrappedArray(35.4, 2467.0))\n",
      "(4,WrappedArray(0.0, 98.0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rankIndexs = Array(2, 4)\n",
       "resultV0 = Map(1 -> WrappedArray(3.0, 15.0), 2 -> WrappedArray(2.0, 1000.0), 3 -> WrappedArray(35.4, 2467.0), 4 -> WrappedArray(0.0, 98.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(1 -> WrappedArray(3.0, 15.0), 2 -> WrappedArray(2.0, 1000.0), 3 -> WrappedArray(35.4, 2467.0), 4 -> WrappedArray(0.0, 98.0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val rankIndexs = \n",
    "// var resultV0 = \n",
    "\n",
    "// for (idx <- 1 to df.schema.length) {\n",
    "    \n",
    "    \n",
    "    \n",
    "// }\n",
    "\n",
    "resultV0.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks Version 1: GroupByKey Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rankIndexs = Array(2, 4)\n",
       "rowLength = 4\n",
       "pairRDD = MapPartitionsRDD[40] at flatMap at <console>:35\n",
       "resultV1 = Map(2 -> Array(35.4, 2467.0), 1 -> Array(2.0, 1000.0), 3 -> Array(0.0, 98.0), 0 -> Array(3.0, 15.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(2 -> [D@33416fef, 1 -> [D@234553d2, 3 -> [D@4a72e0a9, 0 -> [D@7bd123f1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val rankIndexs = \n",
    "// val rowLength = \n",
    "// val pairRDD = \n",
    "// val resultV1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-> 3.0, 15.0\n",
      "1-> 2.0, 1000.0\n",
      "2-> 35.4, 2467.0\n",
      "3-> 0.0, 98.0\n"
     ]
    }
   ],
   "source": [
    "// resultV1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioners and Key/Value Data\n",
    "\n",
    "### Preserving Partitioning Information Across Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partitioner = org.apache.spark.HashPartitioner@4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.HashPartitioner@4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val partitioner = new HashPartitioner(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddA = ShuffledRDD[61] at partitionBy at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[61] at partitionBy at <console>:32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddA = sc.parallelize(Array(1,2,3,4)).map(x => (x, x*1.5)).partitionBy(partitioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(org.apache.spark.HashPartitioner@4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddA.partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddA.map(x => x).partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(org.apache.spark.HashPartitioner@4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddA.mapValues(x => x).partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-located RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a = MapPartitionsRDD[65] at map at <console>:36\n",
       "b = MapPartitionsRDD[67] at map at <console>:37\n",
       "rddA = ShuffledRDD[68] at partitionBy at <console>:39\n",
       "rddB = ShuffledRDD[69] at partitionBy at <console>:41\n",
       "rddC = MapPartitionsRDD[71] at cogroup at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = sc.parallelize(Array(1,2,3,4)).map(x => (x, x*1.5))\n",
    "val b = sc.parallelize(Array(2,4,7,9)).map(x => (x, x*1.5))\n",
    "\n",
    "val rddA = a.partitionBy(partitioner)\n",
    "rddA.cache()\n",
    "val rddB = b.partitionBy(partitioner)\n",
    "rddB.cache()\n",
    "val rddC = a.cogroup(b)\n",
    "rddC.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs Co-partitioned but not Co-Located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a = MapPartitionsRDD[73] at map at <console>:38\n",
       "b = MapPartitionsRDD[75] at map at <console>:39\n",
       "rddA = ShuffledRDD[76] at partitionBy at <console>:41\n",
       "rddB = ShuffledRDD[77] at partitionBy at <console>:43\n",
       "rddC = MapPartitionsRDD[79] at cogroup at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = sc.parallelize(Array(1,2,3,4)).map(x => (x, x*1.5))\n",
    "val b = sc.parallelize(Array(2,4,7,9)).map(x => (x, x*1.5))\n",
    "\n",
    "val rddA = a.partitionBy(partitioner)\n",
    "rddA.cache()\n",
    "val rddB = b.partitionBy(partitioner)\n",
    "rddB.cache()\n",
    "val rddC = a.cogroup(b)\n",
    "rddA.count()\n",
    "rddB.count()\n",
    "rddC.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks Version 2: Secondary Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [Happiness: double, Niceness: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Happiness: double, Niceness: double ... 2 more fields]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq(Panda(15.0, 0.25, 2467.0, 0.0),\n",
    "                                   Panda(2.0, 1000, 35.4, 0.0),\n",
    "                                   Panda(10.0, 2.0, 50.0, 0.0),\n",
    "                                   Panda(3.0, 8.5, 0.2, 98.0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class ColumnIndexPartition\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.Partitioner\n",
    "import scala.math\n",
    "\n",
    "// class ColumnIndexPartition(override val numPartitions: Int) \n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairRDD = MapPartitionsRDD[95] at map at <console>:45\n",
       "partitioner = ColumnIndexPartition@35214ad6\n",
       "sorted = ShuffledRDD[96] at repartitionAndSortWithinPartitions at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[96] at repartitionAndSortWithinPartitions at <console>:47"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val pairRDD = \n",
    "// val partitioner = \n",
    "// val sorted = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0,2.0),1), ((0,3.0),1), ((0,10.0),1), ((0,15.0),1), ((1,0.25),1), ((1,2.0),1), ((1,8.5),1), ((1,1000.0),1), ((2,0.2),1), ((2,35.4),1), ((2,50.0),1), ((2,2467.0),1), ((3,0.0),1), ((3,0.0),1), ((3,0.0),1), ((3,98.0),1)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "targetRanks = Array(2, 4)\n",
       "filterForTargetIndex = MapPartitionsRDD[99] at mapPartitions at <console>:50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[99] at mapPartitions at <console>:50"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val targetRanks = Array(2,4)\n",
    "// val filterForTargetIndex: RDD[(Int, Double)] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultV2 = Map()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// var resultV2 = Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultV2 += 1 -> Seq(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:38: warning: match may not be exhaustive.\n",
       "It would fail on the following input: None\n",
       "       resultV2.get(1) match {\n",
       "                   ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultV2.get(1) match {\n",
    "    case Some(value) => {resultV2 += 1 -> {value ++ Seq(2.0)}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(List(1.0, 2.0))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultV2.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List(1.0, 2.0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1.0) ++ Seq(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,3.0), (0,15.0), (1,2.0), (1,1000.0), (2,35.4), (2,2467.0), (3,0.0), (3,98.0)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterForTargetIndex.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultV2 = Map(0 -> List(3.0, 15.0), 1 -> List(2.0, 1000.0), 2 -> List(35.4, 2467.0), 3 -> List(0.0, 98.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(0 -> List(3.0, 15.0), 1 -> List(2.0, 1000.0), 2 -> List(35.4, 2467.0), 3 -> List(0.0, 98.0))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// var resultV2 = Map[Int, Iterable[Double]]()\n",
    "\n",
    "// })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-> 3.0, 15.0\n",
      "1-> 2.0, 1000.0\n",
      "2-> 35.4, 2467.0\n",
      "3-> 0.0, 98.0\n"
     ]
    }
   ],
   "source": [
    "resultV2.toSeq.sortBy(_._1).foreach({case (key, value) => println(key + \"-> \" + value.mkString(\", \"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks Version 3: Sort on Cell Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [Happiness: double, Niceness: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Happiness: double, Niceness: double ... 2 more fields]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq(Panda(15.0, 0.25, 2467.0, 0.0),\n",
    "                                   Panda(2.0, 1000, 35.4, 0.0),\n",
    "                                   Panda(10.0, 2.0, 50.0, 0.0),\n",
    "                                   Panda(3.0, 8.5, 0.2, 98.0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sortedValueColumnPairs = ShuffledRDD[116] at sortByKey at <console>:44\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[116] at sortByKey at <console>:44"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val sortedValueColumnPairs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.0,3), (0.0,3), (0.0,3), (0.2,2)], [(0.25,1), (2.0,0), (2.0,1), (3.0,0)], [(8.5,1), (10.0,0), (15.0,0), (35.4,2)], [(50.0,2), (98.0,3), (1000.0,1), (2467.0,2)]]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedValueColumnPairs.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numColumns = 4\n",
       "columnsFreqPerPartition = MapPartitionsRDD[131] at mapPartitionsWithIndex at <console>:53\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[131] at mapPartitionsWithIndex at <console>:53"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val numColumns = df.schema.length\n",
    "// val columnsFreqPerPartition = \n",
    "    \n",
    "// })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-> 0, 0, 1, 3\n",
      "1-> 2, 2, 0, 0\n",
      "2-> 2, 1, 1, 0\n",
      "3-> 0, 1, 2, 1\n"
     ]
    }
   ],
   "source": [
    "columnsFreqPerPartition.collect().toSeq.foreach({case (key, value) => println(key + \"-> \" + value.mkString(\", \"))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numColumns = 4\n",
       "targetRanks = List(2, 4)\n",
       "runningTotal = Array(4, 4, 4, 4)\n",
       "ranksLocations = Array((0,List((3,2))), (1,List((0,2), (1,2))), (2,List((0,2), (2,1))), (3,List((1,1), (2,2), (3,1))))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0,List((3,2))), (1,List((0,2), (1,2))), (2,List((0,2), (2,1))), (3,List((1,1), (2,2), (3,1)))]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// import scala.\n",
    "\n",
    "// val numColumns = \n",
    "// val targetRanks = \n",
    "// val runningTotal = \n",
    "// val ranksLocations = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,List((3,2))), (1,List((0,2), (1,2))), (2,List((0,2), (2,1))), (3,List((1,1), (2,2), (3,1)))]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksLocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "targetRanksValues = MapPartitionsRDD[132] at mapPartitionsWithIndex at <console>:65\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[132] at mapPartitionsWithIndex at <console>:65"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// import scala.collection.mutable\n",
    "// val targetRanksValues: RDD[(Int, Double)] = \n",
    "// ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultV3 = Map(2 -> CompactBuffer(35.4, 2467.0), 1 -> CompactBuffer(2.0, 1000.0), 3 -> CompactBuffer(0.0, 98.0), 0 -> CompactBuffer(3.0, 15.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(2 -> CompactBuffer(35.4, 2467.0), 1 -> CompactBuffer(2.0, 1000.0), 3 -> CompactBuffer(0.0, 98.0), 0 -> CompactBuffer(3.0, 15.0))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val resultV3 = targetRanksValues.groupByKey().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-> 3.0, 15.0\n",
      "1-> 2.0, 1000.0\n",
      "2-> 35.4, 2467.0\n",
      "3-> 0.0, 98.0\n"
     ]
    }
   ],
   "source": [
    "resultV3.toSeq.sortBy(_._1).foreach({case (key, value) => println(key + \"-> \" + value.mkString(\", \"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks Version 4: Sort on Cell Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [Happiness: double, Niceness: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Happiness: double, Niceness: double ... 2 more fields]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq(Panda(15.0, 0.25, 2467.0, 0.0),\n",
    "                                   Panda(2.0, 1000, 35.4, 0.0),\n",
    "                                   Panda(10.0, 2.0, 50.0, 0.0),\n",
    "                                   Panda(3.0, 8.5, 0.2, 98.0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sortedValueColumnPairs = ShuffledRDD[139] at sortByKey at <console>:53\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[139] at sortByKey at <console>:53"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sortedValueColumnPairs = df.rdd.flatMap({\n",
    "    row => row.toSeq.zipWithIndex.map{case(v, index) => (v.toString.toDouble, index)}\n",
    "}).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregatedValueColumnRDD = MapPartitionsRDD[135] at mapPartitions at <console>:51\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[135] at mapPartitions at <console>:51"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val aggregatedValueColumnRDD = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sortedAggregatedValueColumnRDD = ShuffledRDD[142] at sortByKey at <console>:53\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[142] at sortByKey at <console>:53"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sortedAggregatedValueColumnRDD = aggregatedValueColumnRDD.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0.0,3),1), ((0.0,3),1), ((0.0,3),1), ((0.2,2),1), ((0.25,1),1), ((2.0,0),1), ((2.0,1),1), ((3.0,0),1), ((8.5,1),1), ((10.0,0),1), ((15.0,0),1), ((35.4,2),1), ((50.0,2),1), ((98.0,3),1), ((1000.0,1),1), ((2467.0,2),1)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedAggregatedValueColumnRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numColumns = 4\n",
       "aggregatedColumnsFreqPerPartition = MapPartitionsRDD[145] at mapPartitionsWithIndex at <console>:61\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[145] at mapPartitionsWithIndex at <console>:61"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val numColumns = df.schema.length\n",
    "// val aggregatedColumnsFreqPerPartition = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-> 0, 0, 1, 3\n",
      "1-> 2, 2, 0, 0\n",
      "2-> 2, 1, 1, 0\n",
      "3-> 0, 1, 2, 1\n"
     ]
    }
   ],
   "source": [
    "aggregatedColumnsFreqPerPartition.collect().toSeq.foreach({case (key, value) => println(key + \"-> \" + value.mkString(\", \"))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numColumns = 4\n",
       "targetRanks = List(2, 4)\n",
       "runningTotal = Array(4, 4, 4, 4)\n",
       "aggregatedRanksLocations = Array((0,List((3,2))), (1,List((0,2), (1,2))), (2,List((0,2), (2,1))), (3,List((1,1), (2,2), (3,1))))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0,List((3,2))), (1,List((0,2), (1,2))), (2,List((0,2), (2,1))), (3,List((1,1), (2,2), (3,1)))]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.MutableList\n",
    "\n",
    "// val numColumns = \n",
    "// val targetRanks = \n",
    "// val runningTotal = \n",
    "// val aggregatedRanksLocations = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,List((3,2))), (1,List((0,2), (1,2))), (2,List((0,2), (2,1))), (3,List((1,1), (2,2), (3,1)))]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregatedRanksLocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asIteratorToIteratorTransformation: (valueColumnPairsIter: Iterator[((Double, Int), Long)], targetsInThisPart: List[(Int, Long)])Iterator[(Int, Double)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// def asIteratorToIteratorTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregatedTargetRanksValues = MapPartitionsRDD[146] at mapPartitionsWithIndex at <console>:73\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[146] at mapPartitionsWithIndex at <console>:73"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// import scala.collection.mutable\n",
    "// val aggregatedTargetRanksValues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultV4 = Map(2 -> CompactBuffer(35.4, 2467.0), 1 -> CompactBuffer(2.0, 1000.0), 3 -> CompactBuffer(0.0, 98.0), 0 -> CompactBuffer(3.0, 15.0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(2 -> CompactBuffer(35.4, 2467.0), 1 -> CompactBuffer(2.0, 1000.0), 3 -> CompactBuffer(0.0, 98.0), 0 -> CompactBuffer(3.0, 15.0))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val resultV4 = aggregatedTargetRanksValues.groupByKey().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-> 3.0, 15.0\n",
      "1-> 2.0, 1000.0\n",
      "2-> 35.4, 2467.0\n",
      "3-> 0.0, 98.0\n"
     ]
    }
   ],
   "source": [
    "resultV4.toSeq.sortBy(_._1).foreach({case (key, value) => println(key + \"-> \" + value.mkString(\", \"))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
