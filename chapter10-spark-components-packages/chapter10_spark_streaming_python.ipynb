{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Spark Streaming\n",
    "\n",
    "In this Chapter, we are going to investigate the streaming capabilities of Spark. \n",
    "\n",
    "In order to perform the exercises included in this Notebook, it is neccesary to send messages to the port 9999. There is a Python script (`send_messages.py`) that performs this automatically. In order to run that script, open a terminal using the Notebook interface. Then, place the working directory in ~/work/chapter10-spark-streaming, and type the following command: `nohup python send_messages.py &`. Please, take note of the process id shown in the terminal. If you want to finish this process, type `kill -9 <process-id>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateless Transformations\n",
    "\n",
    "In this section, we will see a very easy example of some stateless transformations.\n",
    "\n",
    "### Conventional Stateless Transformations\n",
    "\n",
    "The majority of the stateless transformations are almost the same the ones we can applied in conventional RDDs (`map()`, `flatMap()`, `filter()`, ...). In order how to use them, we are going to to an example where we will count the number of occurences of different types of messages (\"info\", \"notice\", \"error\" and \"unkonwn\") coming from a log streaming input. We will perform that count on batch intervals of 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message_type(line):\n",
    "    \"\"\"\n",
    "    Returns the type of message of an input line from log data deppending if the lines\n",
    "    contains the \"[info]\" (type \"info\"), \"[notice]\" (type \"notice\") or \"[error]\" (type \"error\") \n",
    "    keyword. If none of them are found, the returned type is \"unknown\"\n",
    "    \n",
    "    :input line: input line\n",
    "    :return: message type\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"[info]\" in line:\n",
    "        message_type = \"info\"\n",
    "    elif \"[notice]\" in line:\n",
    "        message_type = \"notice\"\n",
    "    elif \"[error]\" in line:\n",
    "        message_type = \"error\"\n",
    "    else:\n",
    "        message_type = \"unknown\"\n",
    "    return message_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark Streaming - Stateless\").master(\"local[*]\").getOrCreate()\n",
    "ssc = StreamingContext(spark.sparkContext, 10)\n",
    "lines = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "codes = lines.map(get_message_type).map(lambda x: (x, 1))\n",
    "codes_count = codes.reduceByKey(lambda x, y: x + y)\n",
    "codes_count.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Stateless Transformations\n",
    "\n",
    "There are also some specific transformations related to Stateless streaming problems. We can highlight the `updateStateByKey` function, that allows to keep some acumulative feautores during batch processing. For example, we are going to perform the last example but mantaining the total numbers of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(newValues, runningCount):\n",
    "    \"\"\"\n",
    "    Accumulates an iterative counter\n",
    "    \"\"\"\n",
    "    \n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark Streaming - Stateless\").master(\"local[*]\").getOrCreate()\n",
    "ssc = StreamingContext(spark.sparkContext, 10)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "lines = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "codes = lines.map(get_message_type).map(lambda x: (x, 1))\n",
    "codes_count = codes.reduceByKey(lambda x, y: x + y)\n",
    "codes_count_cumu = codes_count.updateStateByKey(updateFunction)\n",
    "codes_count_cumu.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful transformations\n",
    "\n",
    "Stateful operations are those which takes into account the values of the current batch and the previous one. The are many equivalent stateless - statuful transformations, where the last ones are characterized by the ending \"AndWindow\". We will see one of the last examples using this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark Streaming - Statefull\").master(\"local[*]\").getOrCreate()\n",
    "sc = StreamingContext(spark.sparkContext, 10)\n",
    "ssc.checkpoint(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "codes = lines.map(get_message_type).map(lambda x: (x, 1))\n",
    "codes_count = codes.reduceByKeyAndWindow(func = lambda x, y: x + y,\n",
    "                                         invFunc = lambda x, y: x - y,\n",
    "                                         windowDuration = 20,\n",
    "                                         slideDuration = 10)\n",
    "codes_count.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
