{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Spark Components and Packages\n",
    "\n",
    "Spark has a great number of libraries to perform different specialized operations. Among them, we can mention Spark Streaming\n",
    "\n",
    "## Stream Processing with Spark\n",
    "\n",
    "DSstreams is an RDD-based API provided by Spark in order to perform streaming processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.streaming.{StreamingContext, Seconds}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DStream repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "lines.repartition(4)\n",
    "lines.print()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DStream repartition with transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "lines.transform{rdd => rdd.repartition(4)}\n",
    "lines.print()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`saveAsTextFiles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "lines.repartition(4)\n",
    "lines.saveAsTextFiles(\"../data/streaming/output1/output\")\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`saveAsSequenceFile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "lines.repartition(4)\n",
    "lines.foreachRDD{(rdd, window) => {\n",
    "    rdd.map(x => (window.toString, x)).saveAsSequenceFile(\"../data/streaming/output2/output_\" + window)}\n",
    "}\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations for Structured Streaming\n",
    "\n",
    "Structured Streaming is a new Dataset-based API offered by Spark in order to perform streaming processing in Datasets.\n",
    "\n",
    "In order to execute the example included above, you have to open a new termila, type the following command `nc -lp 9999 127.0.0.1`. Then you execute the cell below, and afer that, you start writing some texts in the terminal. In this way, we will perform the typical word count example. First, we output the results in the console. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines = spark.readStream\n",
    "  .format(\"socket\")\n",
    "  .option(\"host\", \"127.0.0.1\")\n",
    "  .option(\"port\", 9999)\n",
    "  .load()\n",
    "\n",
    "// Split the lines into words\n",
    "val words = lines.as[String].flatMap(_.split(\" \"))\n",
    "\n",
    "// Generate running word count\n",
    "val wordCounts = words.groupBy(\"value\").count()\n",
    "\n",
    "val query = wordCounts.writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"console\")\n",
    "  .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, we perform a similar example but writing the results to parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines = spark.readStream\n",
    "  .format(\"socket\")\n",
    "  .option(\"host\", \"127.0.0.1\")\n",
    "  .option(\"port\", 9999)\n",
    "  .load()\n",
    "\n",
    "// Split the lines into words\n",
    "val words = lines.as[String].flatMap(_.split(\" \"))\n",
    "\n",
    "// Generate running word count\n",
    "val wordCounts = words.select(\"*\")\n",
    "\n",
    "val query = wordCounts.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", \"../data/streaming/output3\")\n",
    "    .option(\"checkpointLocation\", \"../data/checkpoint\")\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Example using Structured Streaming\n",
    "\n",
    "Data taken from https://docs.databricks.com/spark/latest/mllib/mllib-pipelines-and-stuctured-streaming.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = spark.read.parquet(\"../data/credit_card_fraud_original\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoderEstimator, VectorAssembler}\n",
    "import org.apache.spark.ml.classification.GBTClassifier\n",
    "\n",
    "val oneHot = new OneHotEncoderEstimator()\n",
    "  .setInputCols(Array(\"amountRange\"))\n",
    "  .setOutputCols(Array(\"amountVect\"))\n",
    "\n",
    "val vectorAssembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"amountVect\", \"pcaVector\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val estimator = new GBTClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorSizeHint\n",
    "\n",
    "val vectorSizeHint = new VectorSizeHint()\n",
    "  .setInputCol(\"pcaVector\")\n",
    "  .setSize(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(train, test) = data.randomSplit(weights=Array(.8, .2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(oneHot, vectorSizeHint, vectorAssembler, estimator))\n",
    "\n",
    "val pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testDataPath = \"/tmp/credit-card-frauld-test-data\"\n",
    "\n",
    "test.repartition(20).write\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(testDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.linalg.SQLDataTypes.VectorType\n",
    "\n",
    "val schema = new StructType()\n",
    "  .add(StructField(\"time\", IntegerType))\n",
    "  .add(StructField(\"amountRange\", IntegerType))\n",
    "  .add(StructField(\"label\", IntegerType))\n",
    "  .add(StructField(\"pcaVector\", VectorType))\n",
    "\n",
    "val streamingData = spark.readStream\n",
    "  .schema(schema)\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .parquet(testDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{count, sum, when}\n",
    "\n",
    "val streamingRates = pipelineModel.transform(streamingData)\n",
    "  .groupBy('label)\n",
    "  .agg(\n",
    "    (sum(when('prediction === 'label, 1)) / count('label)).alias(\"true prediction rate\"),\n",
    "    count('label).alias(\"count\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val results = streamingRates.writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"console\")\n",
    "  .start()\n",
    "\n",
    "results.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
