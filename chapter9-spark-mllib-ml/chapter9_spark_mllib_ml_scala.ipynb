{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Spark MLlib and ML\n",
    "\n",
    "In this notebook, we will see the main capabilities of Spark MLlib and ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with MLlilb\n",
    "\n",
    "In this section, we will focus on MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.{DenseVector, SparseVector}\n",
    "import org.apache.spark.mllib.feature.{HashingTF, Word2Vec, IDF, StandardScaler, ChiSqSelector}\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.rdd.RDD\n",
    "import java.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib Feature Encoding and Data Preparation\n",
    "\n",
    "#### Working with Spark Vectors\n",
    "\n",
    "We can create Dense Vectors, Sparse Vectors and Labeled Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denseVector = [1.0,2.0,3.0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1.0,2.0,3.0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val denseVector = new DenseVector(Array(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparseVector = (4,[0,2],[1.5,3.0])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4,[0,2],[1.5,3.0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparseVector = new SparseVector(4, Array(0, 2), Array(1.5, 3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labeledPoint = (1.0,[1.0,2.0,3.0])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.0,[1.0,2.0,3.0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labeledPoint = new LabeledPoint(1, denseVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Textual Data\n",
    "\n",
    "We can also prepare text data using some in-built data transformations capabilities already included in MLlib. We first prepare some text data about Spam and Non-Spam emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iniData = [label: string, text: string ... 3 more fields]\n",
       "iniDataRdd = MapPartitionsRDD[13] at filter at <console>:35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[13] at filter at <console>:35"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iniData = spark.read.option(\"header\", \"true\").csv(\"../data/spam.csv\")\n",
    "val iniDataRdd = iniData.select(\"label\", \"text\").rdd.filter(row => row(0).isInstanceOf[String] && row(1).isInstanceOf[String])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5573"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>ham</td><td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----+-----------------------------------------------------------------------------------------------------------------+\n",
       "| ham | Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat... |\n",
       "+-----+-----------------------------------------------------------------------------------------------------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textRdd = MapPartitionsRDD[14] at map at <console>:37\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[14] at map at <console>:37"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textRdd = iniDataRdd.map(_(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `HashingTF` transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashingTF: (textRdd: org.apache.spark.rdd.RDD[String])org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "Transforms an input RDD of text using the Hashing TF transformer\n",
    "    \n",
    "@input text_rdd: input RDD\n",
    "@return: transformed RDD\n",
    "**/\n",
    "\n",
    "\n",
    "def hashingTF(textRdd: RDD[String]) = {\n",
    "    \n",
    "    val tokenizer = new HashingTF()\n",
    "    val textTokenized = textRdd.map(_.split(\" \").toSeq)\n",
    "    tokenizer.transform(textTokenized)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[14] at map at <console>:37"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashText = MapPartitionsRDD[17] at map at HashingTF.scala:120\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[17] at map at HashingTF.scala:120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hashText = hashingTF(textRdd.map(_.toString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1048576,[17222,138356,181635,201474,293607,318062,362887,416458,443870,527456,550330,589798,665328,704823,708469,755959,790513,846161,907199,1008885],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashText.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashingTFWithText: (textRdd: org.apache.spark.rdd.RDD[String])org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "Transforms an input RDD of text using the Hashing TF transformer\n",
    "keeping also the original text\n",
    "    \n",
    ":input text_rdd: input RDD\n",
    ":return: transformed RDD\n",
    "**/\n",
    "\n",
    "\n",
    "def hashingTFWithText(textRdd: RDD[String]) = {\n",
    "    \n",
    "    val tokenizer = new HashingTF()\n",
    "    textRdd.map(text => (text, tokenizer.transform(text.split(\" \").toSeq)))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashTextPreserving = MapPartitionsRDD[19] at map at <console>:51\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[19] at map at <console>:51"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hashTextPreserving = hashingTFWithText(textRdd.map(_.toString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...,(1048576,[17222,138356,181635,201474,293607,318062,362887,416458,443870,527456,550330,589798,665328,704823,708469,755959,790513,846161,907199,1008885],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashTextPreserving.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `Word2Vec` transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textTokenized = MapPartitionsRDD[20] at map at <console>:39\n",
       "word2vec_trfomer = org.apache.spark.mllib.feature.Word2VecModel@4237d19f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.Word2VecModel@4237d19f"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textTokenized = textRdd.map(_.toString.split(\" \").toSeq)\n",
    "val word2vec_trfomer = new Word2Vec().fit(textTokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02146405540406704,-0.003827216336503625,0.0032874231692403555,0.08191434293985367,-0.05732027068734169,0.009234832599759102,-0.016608454287052155,-0.01469072699546814,0.05036984384059906,-0.027246715500950813,0.09713512659072876,-0.08891306817531586,-0.020493406802415848,0.020492125302553177,-0.01188842672854662,-0.006000001449137926,0.011214228346943855,0.039925310760736465,0.02338774874806404,0.014434332959353924,-0.008040934801101685,0.042061470448970795,-0.03339044377207756,0.024230990558862686,0.013198750093579292,0.057594530284404755,0.09731076657772064,0.010434312745928764,0.019490517675876617,-0.03057178296148777,-0.03533543646335602,0.0036919452250003815,-0.007387078367173672,-0.02520918846130371,0.005467329639941454,0.006146532483398914,0.005227089859545231,0.028425566852092743,0.036847710609436035,-0.04392845556139946,0.03440820053219795,-0.10053680092096329,0.017500048503279686,-0.018390337005257607,-0.015096963383257389,-0.027896001935005188,-0.03566542640328407,0.006895598955452442,-0.025045718997716904,0.07905794680118561,0.04146556928753853,0.07970742136240005,0.06155985966324806,0.04814000427722931,0.02089923806488514,-0.07185856997966766,-0.11791418492794037,0.030139697715640068,0.07671034336090088,0.006001552566885948,-0.12591172754764557,0.01635723188519478,0.0330185741186142,-0.04456649720668793,0.010050453245639801,0.03658970072865486,0.01521521620452404,0.009303138591349125,-0.028465261682868004,1.0738804121501744E-4,-0.08983096480369568,-0.04811835289001465,0.01445593312382698,-0.058696381747722626,-0.005949188489466906,0.052707839757204056,0.02342102862894535,0.015854867175221443,-0.03109482303261757,-0.020627900958061218,-0.059657983481884,-0.08314152806997299,0.04876825958490372,0.04467380419373512,0.027549363672733307,0.04262007772922516,0.028814787045121193,-0.021990850567817688,-0.03334837406873703,0.013313871808350086,-0.007248532027006149,0.033898331224918365,-0.031175460666418076,0.004803002811968327,0.013364835642278194,-0.03970031067728996,-0.011287547647953033,-0.02832374908030033,0.021434033289551735,0.011890164576470852]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_trfomer.transform(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.018583469092845917,-0.02790551632642746,-0.01638362556695938,0.024623297154903412,0.01820003241300583,0.022796304896473885,-0.004462585784494877,0.0018990199314430356,-0.002102164551615715,-0.01185684185475111,0.034062523394823074,-0.017565157264471054,0.013573108240962029,0.008274449966847897,0.02155362069606781,0.0100114606320858,0.029035363346338272,0.01414216123521328,0.021385928615927696,-0.032893091440200806,0.0012255565961822867,0.014664345420897007,-0.010014363564550877,-4.0607567643746734E-4,-0.0035053715109825134,0.017249858006834984,0.018836403265595436,0.01989627629518509,-0.001062321476638317,-0.013529905118048191,-0.021062549203634262,-0.007213293574750423,0.007991934195160866,-0.002972015179693699,-0.01547208707779646,0.011107261292636395,-0.003473537275567651,0.036090198904275894,0.005059196148067713,-0.02550704963505268,5.28823584318161E-4,-0.033462341874837875,0.009227710776031017,0.02203906700015068,0.0155676594004035,-0.00961245410144329,-0.00411571841686964,0.0021625638473778963,-0.003231731476262212,0.05104717239737511,0.0052419849671423435,0.018249832093715668,0.0318920835852623,0.008507841266691685,0.009113393723964691,-0.020025301724672318,-0.02146814949810505,-0.007085160817950964,0.026166129857301712,-0.019984442740678787,-0.004169176798313856,0.006666419096291065,0.013645078055560589,-0.015540596097707748,0.006871213670819998,-0.005247812252491713,-0.0111688869073987,-0.0030598656740039587,-0.002899890299886465,-0.011373626999557018,-0.0370151661336422,-0.01563406176865101,-0.008607318624854088,-0.016947757452726364,5.428821314126253E-4,-0.005387017037719488,0.004965643398463726,0.02393152378499508,-0.051204901188611984,-0.009326768107712269,-0.025855444371700287,-0.02402006834745407,0.021291591227054596,0.026391560211777687,0.017796069383621216,0.03270783647894859,0.009090086445212364,-0.00743368873372674,-0.02856661193072796,-0.01636381819844246,-0.01808304898440838,0.01328472513705492,0.009443632327020168,-0.013396055437624454,0.02276679128408432,0.026897607371211052,-0.027244199067354202,-0.02562250941991806,-0.0049920519813895226,-0.010421380400657654]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_trfomer.transform(\"Free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Data for Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf = org.apache.spark.mllib.feature.HashingTF@40c798d6\n",
       "tfVectors = MapPartitionsRDD[34] at map at HashingTF.scala:120\n",
       "idf = org.apache.spark.mllib.feature.IDF@382f0e89\n",
       "idfModel = org.apache.spark.mllib.feature.IDFModel@eb5f024\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.IDFModel@eb5f024"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf = new HashingTF(100)\n",
    "val tfVectors = tf.transform(textRdd.map(_.toString.split(\" \").toSeq))\n",
    "val idf = new IDF()\n",
    "val idfModel = idf.fit(tfVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spamText = MapPartitionsRDD[37] at map at <console>:37\n",
       "genText = MapPartitionsRDD[39] at map at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[39] at map at <console>:38"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spamText = iniDataRdd.filter(_(0) == \"spam\").map(_(1).toString.split(\" \").toSeq)\n",
    "val genText = iniDataRdd.filter(_(0) != \"spam\").map(_(1).toString.split(\" \").toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spamPoints = MapPartitionsRDD[42] at map at <console>:47\n",
       "genPoints = MapPartitionsRDD[45] at map at <console>:48\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[45] at map at <console>:48"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spamPoints = idfModel.transform(tf.transform(spamText)).map(x => LabeledPoint(1, x))\n",
    "val genPoints = idfModel.transform(tf.transform(genText)).map(x => LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataIni = UnionRDD[46] at union at <console>:50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UnionRDD[46] at union at <console>:50"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataIni = spamPoints.union(genPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlData = MapPartitionsRDD[51] at map at <console>:52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[51] at map at <console>:52"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlData = mlDataIni.map(row => (new Random().nextInt(100), row)).sortByKey().map(_._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,(100,[1,3,18,24,29,34,54,56,61,63,70,73,77,79,86,87,88,89,96],[2.0452290707569443,2.328758888107958,2.067670405229625,2.3342990684835736,1.7012558119933334,1.8036708174214031,2.343601461145887,1.5078520035765604,4.428099880663993,1.6200791887883912,1.1827898336933784,1.7778629334655305,1.6904977620267831,2.1961487300027565,2.379761442560331,2.0719348040160823,2.094422714894262,1.9202291131818907,3.953767316034235]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataSplit = Array(MapPartitionsRDD[52] at randomSplit at <console>:54, MapPartitionsRDD[53] at randomSplit at <console>:54)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[MapPartitionsRDD[52] at randomSplit at <console>:54, MapPartitionsRDD[53] at randomSplit at <console>:54]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataSplit = mlData.randomSplit(Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataTrain = MapPartitionsRDD[52] at randomSplit at <console>:54\n",
       "mlDataTest = MapPartitionsRDD[53] at randomSplit at <console>:54\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[53] at randomSplit at <console>:54"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataTrain = mlDataSplit(0)\n",
    "val mlDataTest = mlDataSplit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[53] at randomSplit at <console>:54"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTrain.cache()\n",
    "mlDataTest.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamPoints.filter(_.label == 1.0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling and Selection\n",
    "\n",
    "It is useful sometimes for the ML algorithms to scale that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler()` --> to scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stdScaler = org.apache.spark.mllib.feature.StandardScaler@398cc281\n",
       "stdScalerModel = org.apache.spark.mllib.feature.StandardScalerModel@1480ccf1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.StandardScalerModel@1480ccf1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.feature.StandardScaler\n",
    "val stdScaler = new StandardScaler()\n",
    "val stdScalerModel = stdScaler.fit(mlData.map(lpoint => lpoint.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainLabel = MapPartitionsRDD[57] at map at <console>:60\n",
       "testLabel = MapPartitionsRDD[58] at map at <console>:61\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[58] at map at <console>:61"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainLabel = mlDataTrain.map(_.label)\n",
    "val testLabel = mlDataTest.map(_.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataTrainScl = MapPartitionsRDD[62] at map at <console>:66\n",
       "mlDataTestScl = MapPartitionsRDD[66] at map at <console>:67\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[66] at map at <console>:67"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataTrainScl = trainLabel.zip(stdScalerModel.transform(mlDataTrain.map(_.features))).map(x => LabeledPoint(x._1, x._2))\n",
    "val mlDataTestScl = testLabel.zip(stdScalerModel.transform(mlDataTest.map(_.features))).map(x => LabeledPoint(x._1, x._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,(100,[1,3,18,24,29,34,54,56,61,63,70,73,77,79,86,87,88,89,96],[2.6092498023853943,2.829417369543754,2.536769897065245,2.9425131453745976,1.8375300213157324,1.895573517843014,2.9834200478287576,1.2772359910016184,5.464175460093429,1.943072888001566,1.3637078242000387,2.0594698576945327,1.8444424730496776,2.732407611455942,3.0927621293082224,2.422815085250892,2.4971212684692707,2.1033897608184167,4.60902425209452]))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTrainScl.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,(100,[7,10,12,14,27,28,32,45,46,47,48,50,70,73,81,82,88,89,90,94],[1.816142313980476,1.4540000671947833,2.9576417198594047,3.2518786697281294,6.389233955248648,2.649568246784961,2.9649557147580787,4.1772288304916545,2.302293243443268,2.9624935495232645,2.665005310919519,2.1102205546562716,2.7274156484000773,4.1189397153890654,1.7994691052982292,2.322198347033588,2.4971212684692707,2.1033897608184167,3.3365463958671873,2.427410364985236]))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTestScl.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ChiSqSelector` --> to select the most relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selector = org.apache.spark.mllib.feature.ChiSqSelector@7dccac1f\n",
       "selectorModel = org.apache.spark.mllib.feature.ChiSqSelectorModel@225555c0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.ChiSqSelectorModel@225555c0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val selector = new ChiSqSelector(100)\n",
    "val selectorModel = selector.fit(mlData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataTrainSel = MapPartitionsRDD[74] at map at <console>:66\n",
       "mlDataTestSel = MapPartitionsRDD[78] at map at <console>:67\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[78] at map at <console>:67"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataTrainSel = trainLabel.zip(selectorModel.transform(mlDataTrain.map(_.features))).map(x => LabeledPoint(x._1, x._2))\n",
    "val mlDataTestSel = testLabel.zip(selectorModel.transform(mlDataTest.map(_.features))).map(x => LabeledPoint(x._1, x._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,(100,[1,3,18,24,29,34,54,56,61,63,70,73,77,79,86,87,88,89,96],[2.0452290707569443,2.328758888107958,2.067670405229625,2.3342990684835736,1.7012558119933334,1.8036708174214031,2.343601461145887,1.5078520035765604,4.428099880663993,1.6200791887883912,1.1827898336933784,1.7778629334655305,1.6904977620267831,2.1961487300027565,2.379761442560331,2.0719348040160823,2.094422714894262,1.9202291131818907,3.953767316034235]))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTrainSel.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,(100,[7,10,12,14,27,28,32,45,46,47,48,50,70,73,81,82,88,89,90,94],[1.7081625982065882,1.3335310318680162,2.2943663581482023,2.5099760825588597,4.830536262034482,2.0341944760332353,2.315949929815377,3.384890364622357,1.8654535169584656,2.2872741298387105,2.207503272105682,1.834646745315708,2.365579667386757,3.555725866931061,1.5870846666533522,1.9742963344521665,2.094422714894262,1.9202291131818907,2.5255492560218293,2.452082104139957]))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTestSel.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib Model Training\n",
    "\n",
    "Once we have prepared our data, we can train some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr = org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS@722ab15b\n",
       "lrModelRaw = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 100, numClasses = 2, threshold = 0.5\n",
       "lrModelScl = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 100, numClasses = 2, threshold = 0.5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 100, numClasses = 2, threshold = 0.5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegressionWithLBFGS()\n",
    "val lrModelRaw = lr.run(mlDataTrain)\n",
    "val lrModelScl = lr.run(mlDataTrainScl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.6524993007948156,0.1175990548428511,-0.0488703785947976,-0.1651541220730223,0.02470329252762825,0.004758313816139249,-0.11783593130735898,-0.34457246840918576,-0.28884598600448225,0.08772671168468467,-0.2887938984421034,0.27356439163820995,-0.09704143642101522,-0.2128740980976528,-0.18330099326038438,0.010570348889996459,-0.4353260899048793,0.33076365329166335,-0.2294716121761005,0.1505773697031762,-0.3160735289385896,0.256080759478671,0.03432091773977774,0.16213303455638062,0.022532573059196967,-0.5417264370889596,-0.3889054235945329,-0.17958489503355157,-0.1587373980407736,-0.39288769049444033,-0.05977237238948764,0.1658118715574863,-0.1873701022231263,-0.07108401663371154,-0.25079926082453435,0.1751939976957695,0.15202210115474463,0.01806048166182609,-0.07799288687007119,0.15719136501327144,0.003723668349210831,-0.06071000114325054,-0.2211974995580636,0.08621116733481554,-0.3425680372636201,-0.40725925276322333,0.19162082567035862,0.1438401844274893,0.09150893770274812,-0.2482110292605332,0.283148011523305,0.16159636855663945,0.09362824287842747,0.05938232931553332,-0.00942108059801553,-0.021681504689695357,-0.48263143853052665,0.06162317217792108,0.06388680581108316,-0.4595494489949539,-0.3977064546588718,0.0697691376218605,-0.6158122984417153,0.3772097989425047,-0.19839248914480664,-0.1499539339484775,0.061637168890731876,0.3695943717281687,-0.3821732297773752,0.2756099686239339,0.1261670557814658,-0.4351863543176958,-0.14836857158056926,0.10723047464307577,-0.1917003568398575,-0.12744464975438932,-0.37095064665964145,-0.3493178662362073,0.08820504413457676,0.18222723295932675,-0.00387988471361117,-0.14910128802030476,0.09877603642178702,-0.17119188439619662,-0.11677420604451452,0.07760095696706375,-0.1691826226786802,0.5254426728764898,0.5370035055187402,0.32288394292003114,0.16313900981621415,-0.12133655094259664,-0.18221501083068684,-0.43492842905974727,-0.0726055457826111,-0.24225423220918038,-0.07545353834025033,-0.16727808620177342,0.12174688524409176,0.17113113213233658]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModelRaw.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.591629123336054,0.09217860453156451,-0.03688180704739261,-0.13593050421799519,0.020624901401583958,0.0037757674113313377,-0.09371466075235703,-0.32408572741102576,-0.22953519488042023,0.06912053040416696,-0.2648663050818764,0.20484554018184728,-0.07527910009375365,-0.17246497417580461,-0.14148163437823705,0.00825876414198023,-0.3821359397858671,0.2727954534686777,-0.18703772142903155,0.12444576318386746,-0.2720054360159467,0.21274831379570136,0.028211518470854662,0.13128022756486366,0.01787511616907382,-0.5010312463415187,-0.38642165047390525,-0.13577392120077264,-0.12186994565105402,-0.36375050157587924,-0.0487498787716791,0.14194302201198117,-0.14635624165757652,-0.06357667476898972,-0.2386398119207753,0.13598647308860967,0.13904043446199219,0.014901904016705641,-0.06617629073856457,0.124831060069052,0.0030179996657308515,-0.04715631048494058,-0.19579456470477005,0.07475067094100905,-0.28239450202221533,-0.3300101518305582,0.15526247326975148,0.11105574650962002,0.07579957855171389,-0.18510225157258375,0.24617169832679836,0.11651517719797337,0.07510110411623255,0.04642744879197653,-0.007400653579151363,-0.01768860665913328,-0.5697747219028642,0.048281993149669515,0.05351162755444315,-0.3783130475418847,-0.3481675869494834,0.05654004199421216,-0.55215414015613,0.3145068560460872,-0.15826919289041844,-0.11630902005443261,0.05065110076642406,0.2910852365593296,-0.3196866928347461,0.24391521073164876,0.10942894678549502,-0.37375674750592824,-0.1672961505518519,0.0925680390482832,-0.15402844461394066,-0.09914795168331698,-0.38619848605174645,-0.32016236870315484,0.07573874519078046,0.14646354539405154,-0.003289637641040486,-0.13150343470670092,0.08397782510195238,-0.13799841272376198,-0.09354903969459849,0.059515607696067827,-0.13017951765078603,0.4493462865059401,0.450403572360653,0.2947675979618749,0.12348565132493101,-0.09405900882137926,-0.14478940912429009,-0.3738700693111659,-0.07334349479715441,-0.20589826508368603,-0.06472644044631946,-0.14103453034923513,0.09817113726409929,0.14799883016554932]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModelScl.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Once the model is trained, we can perform predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawPreds = MapPartitionsRDD[169] at mapPartitions at GeneralizedLinearAlgorithm.scala:70\n",
       "sclPreds = MapPartitionsRDD[171] at mapPartitions at GeneralizedLinearAlgorithm.scala:70\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[171] at mapPartitions at GeneralizedLinearAlgorithm.scala:70"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawPreds = lrModelRaw.predict(mlDataTest.map(_.features))\n",
    "val sclPreds = lrModelScl.predict(mlDataTestScl.map(_.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawPreds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclPreds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving and Persistence\n",
    "\n",
    "Many times, once we train our model, we save it and the load it in oder programs to make predictions. We try first the internal format of Spark, which allows us to save and load a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"rm -rf ../data/lrModelRaw\".!\n",
    "lrModelRaw.save(sc, \"../data/lrModelRaw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModelRawLoaded = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 100, numClasses = 2, threshold = 0.5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 100, numClasses = 2, threshold = 0.5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModelRawLoaded = LogisticRegressionModel.load(sc, \"../data/lrModelRaw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawPredsLoaded = MapPartitionsRDD[188] at mapPartitions at GeneralizedLinearAlgorithm.scala:70\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[188] at mapPartitions at GeneralizedLinearAlgorithm.scala:70"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawPredsLoaded = lrModelRawLoaded.predict(mlDataTest.map(_.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawPredsLoaded.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "MLlib includes some functionalities to calculate automatically some metrics of trained ML models. While there are more, here we will evaluate the LR model of the spam classification section using the `BinaryClassificationMetrics` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,(100,[1,3,18,24,29,34,54,56,61,63,70,73,77,79,86,87,88,89,96],[2.0452290707569443,2.328758888107958,2.067670405229625,2.3342990684835736,1.7012558119933334,1.8036708174214031,2.343601461145887,1.5078520035765604,4.428099880663993,1.6200791887883912,1.1827898336933784,1.7778629334655305,1.6904977620267831,2.1961487300027565,2.379761442560331,2.0719348040160823,2.094422714894262,1.9202291131818907,3.953767316034235]))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTrain.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModelEval = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 100, numClasses = 2, threshold = 0.5\n",
       "predLabelLr = MapPartitionsRDD[189] at map at <console>:67\n",
       "metricsLr = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@1b8098d3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@1b8098d3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModelEval = lrModelRaw\n",
    "val predLabelLr = mlDataTest.map{case LabeledPoint(label, features) => (lrModelEval.predict(features), label)}\n",
    "val metricsLr = new BinaryClassificationMetrics(predLabelLr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predLabelLr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTest.filter(_.label == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predLabelLr.filter(x => x._1 == 0.0 && x._2 == 0.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predLabelLr.filter(x => x._1 == 0.0 && x._2 == 0.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR model\n",
      "Area Under PR: 0.35890776165347404\n",
      "Area Under ROC: 0.704864593781344\n"
     ]
    }
   ],
   "source": [
    "println(\"LR model\")\n",
    "println(\"Area Under PR: \" + metricsLr.areaUnderPR)\n",
    "println(\"Area Under ROC: \" + metricsLr.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Spark ML\n",
    "\n",
    "Now, we are going to see some of the capabilities offered by the Spark ML package, which works with DataFrames instead that MLlib that works with RDDs. In particular, we are going to do again the spam classification problem using two Pipelines: one for the data preparation and the other one for the ML model.\n",
    "\n",
    "### Data Preparation: Data Encoding & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer, IDF, SQLTransformer, StringIndexer, VectorAssembler, StandardScaler}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+----+----+\n",
      "|label|                text| _c2| _c3| _c4|\n",
      "+-----+--------------------+----+----+----+\n",
      "|  ham|Go until jurong p...|null|null|null|\n",
      "|  ham|Ok lar... Joking ...|null|null|null|\n",
      "| spam|Free entry in 2 a...|null|null|null|\n",
      "|  ham|U dun say so earl...|null|null|null|\n",
      "|  ham|Nah I don't think...|null|null|null|\n",
      "| spam|FreeMsg Hey there...|null|null|null|\n",
      "|  ham|Even my brother i...|null|null|null|\n",
      "|  ham|As per your reque...|null|null|null|\n",
      "| spam|WINNER!! As a val...|null|null|null|\n",
      "| spam|Had your mobile 1...|null|null|null|\n",
      "|  ham|I'm gonna be home...|null|null|null|\n",
      "| spam|SIX chances to wi...|null|null|null|\n",
      "| spam|URGENT! You have ...|null|null|null|\n",
      "|  ham|I've been searchi...|null|null|null|\n",
      "|  ham|I HAVE A DATE ON ...|null|null|null|\n",
      "| spam|XXXMobileMovieClu...|null|null|null|\n",
      "|  ham|Oh k...i'm watchi...|null|null|null|\n",
      "|  ham|Eh u remember how...|null|null|null|\n",
      "|  ham|Fine if that��s t...|null|null|null|\n",
      "| spam|England v Macedon...|null|null|null|\n",
      "+-----+--------------------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iniData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlSelect = sql_065623af654d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sql_065623af654d"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlSelect = new SQLTransformer().setStatement(\"SELECT label, text FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlFilter = sql_d3f2652b3ad0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sql_d3f2652b3ad0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlFilter = new SQLTransformer().setStatement(\"SELECT * from __THIS__ WHERE text is not null AND label is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelIndexer = strIdx_2317a3b760c7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "strIdx_2317a3b760c7"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\"label_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer = tok_60ef9752fd72\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tok_60ef9752fd72"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"text_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countText = sql_757a1d37c6b5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sql_757a1d37c6b5"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val countText = new SQLTransformer().setStatement(\"SELECT *, size(text_token) as count FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf = hashingTF_5aeba64632e4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hashingTF_5aeba64632e4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf = new HashingTF().setNumFeatures(1000).setInputCol(\"text_token\").setOutputCol(\"text_tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf = idf_ea3a95c144a8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "idf_ea3a95c144a8"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF().setInputCol(\"text_tf\").setOutputCol(\"text_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler = vecAssembler_282aebb1e6a9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vecAssembler_282aebb1e6a9"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"text_features\", \"count\")).setOutputCol(\"features_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler = stdScal_5a99e5855034\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "stdScal_5a99e5855034"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler = new StandardScaler().setInputCol(\"features_raw\").setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "etlPipelineModel = pipeline_a8ca84d42fbd\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_a8ca84d42fbd"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val etlPipelineModel = new Pipeline().setStages(Array(sqlSelect, sqlFilter, labelIndexer, \n",
    "                                      tokenizer, countText, tf, idf, assembler, scaler)).fit(iniData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataPipe = [label: string, text: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: string, text: string ... 7 more fields]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataPipe = etlPipelineModel.transform(iniData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  ham|(1001,[7,77,150,1...|\n",
      "|  ham|(1001,[20,316,484...|\n",
      "| spam|(1001,[30,35,73,1...|\n",
      "|  ham|(1001,[57,368,372...|\n",
      "|  ham|(1001,[135,163,32...|\n",
      "| spam|(1001,[25,36,91,9...|\n",
      "|  ham|(1001,[18,47,48,5...|\n",
      "|  ham|(1001,[36,71,92,2...|\n",
      "| spam|(1001,[39,43,61,7...|\n",
      "| spam|(1001,[36,73,82,1...|\n",
      "|  ham|(1001,[26,41,106,...|\n",
      "| spam|(1001,[15,35,36,4...|\n",
      "| spam|(1001,[68,73,122,...|\n",
      "|  ham|(1001,[19,36,39,1...|\n",
      "|  ham|(1001,[44,82,170,...|\n",
      "| spam|(1001,[41,43,49,6...|\n",
      "|  ham|(1001,[275,426,44...|\n",
      "|  ham|(1001,[80,147,236...|\n",
      "|  ham|(1001,[159,170,29...|\n",
      "| spam|(1001,[9,19,45,71...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlDataPipe.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataPipeSplits = Array([label: string, text: string ... 7 more fields], [label: string, text: string ... 7 more fields])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[label: string, text: string ... 7 more fields], [label: string, text: string ... 7 more fields]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataPipeSplits = mlDataPipe.randomSplit(Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataPipeTrain = [label: string, text: string ... 7 more fields]\n",
       "mlDataPipeTest = [label: string, text: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: string, text: string ... 7 more fields]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataPipeTrain = mlDataPipeSplits(0)\n",
    "val mlDataPipeTest = mlDataPipeSplits(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4391"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataPipeTrain.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1182"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataPipeTest.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark ML Models\n",
    "\n",
    "Once we have our data cleaned and encoded, we can now train a ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr = logreg_1f358ec290e4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "logreg_1f358ec290e4"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression().setFeaturesCol(\"features\").setLabelCol(\"label_num\").setRegParam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrPipeline = pipeline_bf6597265858\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_bf6597265858"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrPipeline = new Pipeline().setStages(Array(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrPipelineModel = pipeline_bf6597265858\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_bf6597265858"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrPipelineModel = lrPipeline.fit(mlDataPipeTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in any other pipeline, we can access to one of the steps, and its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "\tlogreg_1f358ec290e4-aggregationDepth: 2,\n",
       "\tlogreg_1f358ec290e4-elasticNetParam: 0.0,\n",
       "\tlogreg_1f358ec290e4-family: auto,\n",
       "\tlogreg_1f358ec290e4-featuresCol: features,\n",
       "\tlogreg_1f358ec290e4-fitIntercept: true,\n",
       "\tlogreg_1f358ec290e4-labelCol: label_num,\n",
       "\tlogreg_1f358ec290e4-maxIter: 100,\n",
       "\tlogreg_1f358ec290e4-predictionCol: prediction,\n",
       "\tlogreg_1f358ec290e4-probabilityCol: probability,\n",
       "\tlogreg_1f358ec290e4-rawPredictionCol: rawPrediction,\n",
       "\tlogreg_1f358ec290e4-regParam: 0.1,\n",
       "\tlogreg_1f358ec290e4-standardization: true,\n",
       "\tlogreg_1f358ec290e4-threshold: 0.5,\n",
       "\tlogreg_1f358ec290e4-tol: 1.0E-6\n",
       "}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrPipelineModel.stages(0).extractParamMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Persistence and Spark ML\n",
    "\n",
    "We can save and load our pipelines (including both data transformers and ML algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.io.IOException\n",
       "Message: Path ../data/lrPipelineModel already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n",
       "StackTrace:   at org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:503)\n",
       "  at org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:102)\n",
       "  at org.apache.spark.ml.util.MLWritable$class.save(ReadWrite.scala:162)\n",
       "  at org.apache.spark.ml.PipelineModel.save(Pipeline.scala:293)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"rm -rf ../data/etlPipelineModel\".!\n",
    "\"rm -rf ../data/lr_pipeline_model\".!\n",
    "etlPipelineModel.save(\"../data/etlPipelineModel\")\n",
    "lrPipelineModel.save(\"../data/lrPipelineModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "etlPipelineLoad = pipeline_a8ca84d42fbd\n",
       "lrPipelineLoad = pipeline_5f86381e1e35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_5f86381e1e35"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val etlPipelineLoad = PipelineModel.load(\"../data/etlPipelineModel\")\n",
    "val lrPipelineLoad = PipelineModel.load(\"../data/lrPipelineModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions = [label: string, text: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: string, text: string ... 10 more fields]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = lrPipelineLoad.transform(etlPipelineLoad.transform(iniData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|label_num|prediction|\n",
      "+---------+----------+\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"label_num\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automated Model Selection: Parameter Search\n",
    "\n",
    "Spark ML offers some functionalities to perform hiperparameter tunning on ML models. Let's check our previous problem testing different regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr = logreg_c1ba64163252\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "logreg_c1ba64163252"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression().setFeaturesCol(\"features\").setLabelCol(\"label_num\").setRegParam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimatorPipeline = pipeline_58193ef5e226\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_58193ef5e226"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val estimatorPipeline = new Pipeline().setStages(Array(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\tlogreg_c1ba64163252-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_c1ba64163252-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_c1ba64163252-regParam: 0.05\n",
       "})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "\tlogreg_c1ba64163252-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_c1ba64163252-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_c1ba64163252-regParam: 0.05\n",
       "}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder().addGrid(lr.regParam, Array(0.1, 0.01, 0.05)).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator = binEval_6b19b7eaa0cf\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "binEval_6b19b7eaa0cf"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator().setLabelCol(\"label_num\").setRawPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossVal = cv_cb1f28a7c907\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cv_cb1f28a7c907"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossVal = new CrossValidator().setEstimator(estimatorPipeline).setEstimatorParamMaps(paramGrid)\n",
    ".setEvaluator(evaluator).setNumFolds(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cvModel = cv_cb1f28a7c907\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cv_cb1f28a7c907"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cvModel = crossVal.fit(mlDataPipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|label_num|prediction|\n",
      "+---------+----------+\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      0.0|       0.0|\n",
      "|      1.0|       1.0|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvModel.transform(mlDataPipe).select(\"label_num\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
