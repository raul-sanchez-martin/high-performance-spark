{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Joins (SQL and Core)\n",
    "\n",
    "In this chapter, we will study joins in Spark, both in the Core and SQL API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Joins-SQL-core\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First, we create some RDD and DataFrames for the rest of the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_accounts_rdd = sc.parallelize([(1, (\"John\", 11)), (2, (\"Isabelle\", 22)), (3, (\"Maria\", 33)),\n",
    "                                      (4, (\"Peter\", 44)), (5, (\"Connor\", 55)), (6, (\"Max\", 66))])\n",
    "\n",
    "people_accounts_schema = StructType([StructField(\"id\", IntegerType(), False),\n",
    "                                     StructField(\"Name\", StringType(), False),\n",
    "                                     StructField(\"account_id\", IntegerType(), False)])\n",
    "\n",
    "people_accounts_df = spark.createDataFrame(people_accounts_rdd.map(lambda x: (x[0], x[1][0], x[1][1])), \n",
    "                                           people_accounts_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('John', 11)), (2, ('Isabelle', 22)), (3, ('Maria', 33))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_accounts_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+\n",
      "| id|    Name|account_id|\n",
      "+---+--------+----------+\n",
      "|  1|    John|        11|\n",
      "|  2|Isabelle|        22|\n",
      "|  3|   Maria|        33|\n",
      "|  4|   Peter|        44|\n",
      "|  5|  Connor|        55|\n",
      "|  6|     Max|        66|\n",
      "+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_accounts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_emails_rdd = sc.parallelize([(1, \"john@gmail.com\"), (3, \"maria@gmail.com\"),\n",
    "                                    (4, \"peter@gmail.com\"), (5, \"connor@gmail.com\")])\n",
    "\n",
    "people_emails_schema = StructType([StructField(\"id\", IntegerType(), False),\n",
    "                                     StructField(\"Email\", StringType(), False)])\n",
    "\n",
    "people_emails_df = spark.createDataFrame(people_emails_rdd, \n",
    "                                         people_emails_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'john@gmail.com'), (3, 'maria@gmail.com'), (4, 'peter@gmail.com')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_emails_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|           Email|\n",
      "+---+----------------+\n",
      "|  1|  john@gmail.com|\n",
      "|  3| maria@gmail.com|\n",
      "|  4| peter@gmail.com|\n",
      "|  5|connor@gmail.com|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_emails_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_balance_type_rdd = sc.parallelize([(11, (152.0, 1)), (22, (3545.3, 2)), (33, (12.5, 1)),\n",
    "                                            (44, (75.0, 1)), (55, (4853.12, 2)), (66, (47.0, 1))])\n",
    "\n",
    "accounts_balance_type_schema = StructType([StructField(\"account_id\", IntegerType(), False),\n",
    "                                           StructField(\"balance\", FloatType(), False),\n",
    "                                           StructField(\"account_type_id\", IntegerType(), False)])\n",
    "\n",
    "accounts_balance_type_df = spark.createDataFrame(accounts_balance_type_rdd.map(lambda x: (x[0], x[1][0], x[1][1])),\n",
    "                                           accounts_balance_type_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, (152.0, 1)), (22, (3545.3, 2)), (33, (12.5, 1))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accounts_balance_type_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------+\n",
      "|account_id|balance|account_type_id|\n",
      "+----------+-------+---------------+\n",
      "|        11|  152.0|              1|\n",
      "|        22| 3545.3|              2|\n",
      "|        33|   12.5|              1|\n",
      "|        44|   75.0|              1|\n",
      "|        55|4853.12|              2|\n",
      "|        66|   47.0|              1|\n",
      "+----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_balance_type_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_type_description_rdd = sc.parallelize([(1, \"Basic Account\"), (2, \"Premium Account\")])\n",
    "\n",
    "accounts_type_description_schema = StructType([StructField(\"account_type_id\", IntegerType(), False),\n",
    "                                               StructField(\"account_description\", StringType(), False)])\n",
    "\n",
    "accounts_type_description_df = spark.createDataFrame(accounts_type_description_rdd, accounts_type_description_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Basic Account'), (2, 'Premium Account')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accounts_type_description_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+\n",
      "|account_type_id|account_description|\n",
      "+---------------+-------------------+\n",
      "|              1|      Basic Account|\n",
      "|              2|    Premium Account|\n",
      "+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_type_description_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Spark Joins\n",
    "\n",
    "We will start with joins of Key / Value RDDs. We can distinguish `join`, `leftOuterJoin` and `rightOuterJoin` joins.\n",
    "\n",
    "`join()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (('John', 11), 'john@gmail.com')),\n",
       " (3, (('Maria', 33), 'maria@gmail.com')),\n",
       " (4, (('Peter', 44), 'peter@gmail.com')),\n",
       " (5, (('Connor', 55), 'connor@gmail.com'))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_accounts_rdd.join(people_emails_rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'John', 'john@gmail.com'),\n",
       " (3, 'Maria', 'maria@gmail.com'),\n",
       " (4, 'Peter', 'peter@gmail.com'),\n",
       " (5, 'Connor', 'connor@gmail.com')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_accounts_rdd.join(people_emails_rdd).map(lambda x: (x[0], x[1][0][0], x[1][1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`leftOuterJoin()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'John', 'john@gmail.com'),\n",
       " (2, 'Isabelle', None),\n",
       " (3, 'Maria', 'maria@gmail.com'),\n",
       " (4, 'Peter', 'peter@gmail.com'),\n",
       " (5, 'Connor', 'connor@gmail.com'),\n",
       " (6, 'Max', None)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_accounts_rdd.leftOuterJoin(people_emails_rdd).map(lambda x: (x[0], x[1][0][0], x[1][1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up join processes, specially if one same RDD is going to be joined several times, it is useful to pre-partition the RDDs.\n",
    "\n",
    "Let's check if the `people_accounts_rdd` has any partitioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(people_accounts_rdd.partitioner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform a partition on that data by key, into 2 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_accounts_par = people_accounts_rdd.partitionBy(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now the partitioner of the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.rdd.Partitioner object at 0x7f588e588be0>\n"
     ]
    }
   ],
   "source": [
    "print(people_accounts_par.partitioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_emails_par = people_emails_rdd.partitionBy(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform 10 different joins without any kind of pre-partition on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join time: 0.4817535877227783\n",
      "Join time: 0.4499666690826416\n",
      "Join time: 0.45418882369995117\n",
      "Join time: 0.3062405586242676\n",
      "Join time: 0.3055107593536377\n",
      "Join time: 0.3097646236419678\n",
      "Join time: 0.32379674911499023\n",
      "Join time: 0.318495512008667\n",
      "Join time: 0.2895677089691162\n",
      "Join time: 0.30077052116394043\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for idx in range(10):\n",
    "    ini_time = time.time()\n",
    "    people_accounts_rdd.join(people_emails_rdd).collect()\n",
    "    print(\"Join time: {0}\".format(time.time() - ini_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all the times are more or less the same. Now, we repeat the process but using pre-partition RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join time: 0.24150776863098145\n",
      "Join time: 0.11041045188903809\n",
      "Join time: 0.11476993560791016\n",
      "Join time: 0.11603760719299316\n",
      "Join time: 0.1255946159362793\n",
      "Join time: 0.1141667366027832\n",
      "Join time: 0.12054777145385742\n",
      "Join time: 0.11200523376464844\n",
      "Join time: 0.11954164505004883\n",
      "Join time: 0.11143922805786133\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for idx in range(10):\n",
    "    ini_time = time.time()\n",
    "    people_accounts_par.join(people_emails_par).collect()\n",
    "    print(\"Join time: {0}\".format(time.time() - ini_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, once the first join has been done (which is already faster than the previous ones), the join time is reduced very significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when joining large RDDs with small RDDs, it is quiet convinient to \"broadcast\" the small RDDs to all the executors. Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_rdd = accounts_balance_type_rdd.map(lambda x: (x[1][1], (x[0], x[1][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (11, 152.0)),\n",
       " (2, (22, 3545.3)),\n",
       " (1, (33, 12.5)),\n",
       " (1, (44, 75.0)),\n",
       " (2, (55, 4853.12)),\n",
       " (1, (66, 47.0))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_rdd_local = accounts_type_description_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Basic Account', 2: 'Premium Account'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_rdd_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_rdd_local_bcast = sc.broadcast(small_rdd_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcastJoinFunction(sec_iterator):\n",
    "    for sec_iter in sec_iterator:\n",
    "        yield (sec_iter[0], (sec_iter[1][0], sec_iter[1][1], small_rdd_local_bcast.value.get(sec_iter[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (11, 152.0, 'Basic Account')),\n",
       " (2, (22, 3545.3, 'Premium Account')),\n",
       " (1, (33, 12.5, 'Basic Account')),\n",
       " (1, (44, 75.0, 'Basic Account')),\n",
       " (2, (55, 4853.12, 'Premium Account')),\n",
       " (1, (66, 47.0, 'Basic Account'))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_rdd.mapPartitions(broadcastJoinFunction).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL \n",
    "\n",
    "Joining DataFrames using the SQL API is quiet simple and efficient. We can highlight the following joining modes: `inner`, `left_outer`, `right_outer`, `outer`, `left_semi` and `left_anti`. Let's see some examples of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----------------+\n",
      "| id|  Name|account_id|           Email|\n",
      "+---+------+----------+----------------+\n",
      "|  1|  John|        11|  john@gmail.com|\n",
      "|  3| Maria|        33| maria@gmail.com|\n",
      "|  5|Connor|        55|connor@gmail.com|\n",
      "|  4| Peter|        44| peter@gmail.com|\n",
      "+---+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_accounts_df.join(people_emails_df, \"id\", \"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`left_outer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+----------------+\n",
      "| id|    Name|account_id|           Email|\n",
      "+---+--------+----------+----------------+\n",
      "|  1|    John|        11|  john@gmail.com|\n",
      "|  6|     Max|        66|            null|\n",
      "|  3|   Maria|        33| maria@gmail.com|\n",
      "|  5|  Connor|        55|connor@gmail.com|\n",
      "|  4|   Peter|        44| peter@gmail.com|\n",
      "|  2|Isabelle|        22|            null|\n",
      "+---+--------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_accounts_df.join(people_emails_df, \"id\", \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rigth_outer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----------------+\n",
      "| id|  Name|account_id|           Email|\n",
      "+---+------+----------+----------------+\n",
      "|  1|  John|        11|  john@gmail.com|\n",
      "|  3| Maria|        33| maria@gmail.com|\n",
      "|  5|Connor|        55|connor@gmail.com|\n",
      "|  4| Peter|        44| peter@gmail.com|\n",
      "+---+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_accounts_df.join(people_emails_df, \"id\", \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`full`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+----------------+\n",
      "| id|    Name|account_id|           Email|\n",
      "+---+--------+----------+----------------+\n",
      "|  1|    John|        11|  john@gmail.com|\n",
      "|  6|     Max|        66|            null|\n",
      "|  3|   Maria|        33| maria@gmail.com|\n",
      "|  5|  Connor|        55|connor@gmail.com|\n",
      "|  4|   Peter|        44| peter@gmail.com|\n",
      "|  2|Isabelle|        22|            null|\n",
      "+---+--------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_accounts_df.join(people_emails_df, \"id\", \"full\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`left_semi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|  Name|account_id|\n",
      "+---+------+----------+\n",
      "|  1|  John|        11|\n",
      "|  3| Maria|        33|\n",
      "|  5|Connor|        55|\n",
      "|  4| Peter|        44|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_accounts_df.join(people_emails_df, \"id\", \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`left_anti`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+\n",
      "| id|    Name|account_id|\n",
      "+---+--------+----------+\n",
      "|  6|     Max|        66|\n",
      "|  2|Isabelle|        22|\n",
      "+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_accounts_df.join(people_emails_df, \"id\", \"left_anti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting option is the self join capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+---+--------+----------+\n",
      "| id|    Name|account_id| id|    Name|account_id|\n",
      "+---+--------+----------+---+--------+----------+\n",
      "|  1|    John|        11|  1|    John|        11|\n",
      "|  6|     Max|        66|  6|     Max|        66|\n",
      "|  3|   Maria|        33|  3|   Maria|        33|\n",
      "|  5|  Connor|        55|  5|  Connor|        55|\n",
      "|  4|   Peter|        44|  4|   Peter|        44|\n",
      "|  2|Isabelle|        22|  2|Isabelle|        22|\n",
      "+---+--------+----------+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_accounts_df.join(people_accounts_df, people_accounts_df[\"id\"] == people_accounts_df[\"id\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also make use of broadcast joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-------+-------------------+\n",
      "|account_type_id|account_id|balance|account_description|\n",
      "+---------------+----------+-------+-------------------+\n",
      "|              1|        11|  152.0|      Basic Account|\n",
      "|              2|        22| 3545.3|    Premium Account|\n",
      "|              1|        33|   12.5|      Basic Account|\n",
      "|              1|        44|   75.0|      Basic Account|\n",
      "|              2|        55|4853.12|    Premium Account|\n",
      "|              1|        66|   47.0|      Basic Account|\n",
      "+---------------+----------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_balance_type_df.join(F.broadcast(accounts_type_description_df), \"account_type_id\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
