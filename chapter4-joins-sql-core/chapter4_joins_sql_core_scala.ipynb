{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Joins (SQL and Core)\n",
    "\n",
    "In this chapter, we will study joins in Spark, both in the Core and SQL API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First, we create some RDD and DataFrames for the rest of the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, \n",
    "                                   StringType, IntegerType, DoubleType}\n",
    "\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class PersonAccount\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class PersonAccount(id: Int, name: String, account_id: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleAccountsRdd = ParallelCollectionRDD[0] at parallelize at <console>:33\n",
       "peopleAccountsSchema = StructType(StructField(id,IntegerType,false), StructField(Name,StringType,false), StructField(account_id,IntegerType,false))\n",
       "peopleAccountsDf = [id: int, Name: string ... 1 more field]\n",
       "peopleAccountsDs = [id: int, name: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 1 more field]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleAccountsRdd = sc.parallelize(Array((1, (\"John\", 11)), (2, (\"Isabelle\", 22)), (3, (\"Maria\", 33)),\n",
    "                                      (4, (\"Peter\", 44)), (5, (\"Connor\", 55)), (6, (\"Max\", 66))))\n",
    "\n",
    "val peopleAccountsSchema = new StructType(Array(StructField(\"id\", IntegerType, false),\n",
    "                                                StructField(\"Name\", StringType, false),\n",
    "                                                StructField(\"account_id\", IntegerType, false)))\n",
    "\n",
    "val peopleAccountsDf = spark.createDataFrame(peopleAccountsRdd.map(x=> Row(x._1, x._2._1, x._2._2)),\n",
    "                                             peopleAccountsSchema)\n",
    "\n",
    "val peopleAccountsDs = spark.createDataset(peopleAccountsRdd.map(x=> PersonAccount(x._1, x._2._1, x._2._2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,(John,11)), (2,(Isabelle,22))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleAccountsRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+\n",
      "| id|    Name|account_id|\n",
      "+---+--------+----------+\n",
      "|  1|    John|        11|\n",
      "|  2|Isabelle|        22|\n",
      "|  3|   Maria|        33|\n",
      "|  4|   Peter|        44|\n",
      "|  5|  Connor|        55|\n",
      "|  6|     Max|        66|\n",
      "+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleAccountsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class PersonEmail\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class PersonEmail(id: Int, email: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleEmailsRdd = ParallelCollectionRDD[8] at parallelize at <console>:33\n",
       "peopleEmailsSchema = StructType(StructField(id,IntegerType,false), StructField(Email,StringType,false))\n",
       "peopleEmailsDf = [id: int, Email: string]\n",
       "peopleEmailsDs = [id: int, email: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, email: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleEmailsRdd = sc.parallelize(Array((1, \"john@gmail.com\"), (3, \"maria@gmail.com\"),\n",
    "                                           (4, \"peter@gmail.com\"), (5, \"connor@gmail.com\")))\n",
    "\n",
    "val peopleEmailsSchema = new StructType(Array(StructField(\"id\", IntegerType, false),\n",
    "                                              StructField(\"Email\", StringType, false)))\n",
    "\n",
    "val peopleEmailsDf = spark.createDataFrame(peopleEmailsRdd.map(x=> Row(x._1, x._2)),\n",
    "                                           peopleEmailsSchema)\n",
    "\n",
    "val peopleEmailsDs = spark.createDataFrame(peopleEmailsRdd.map(x=> PersonEmail(x._1, x._2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,john@gmail.com), (3,maria@gmail.com)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleEmailsRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|           Email|\n",
      "+---+----------------+\n",
      "|  1|  john@gmail.com|\n",
      "|  3| maria@gmail.com|\n",
      "|  4| peter@gmail.com|\n",
      "|  5|connor@gmail.com|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleEmailsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accountsBalanceTypeRdd = ParallelCollectionRDD[16] at parallelize at <console>:31\n",
       "accountsBalanceTypeSchema = StructType(StructField(account_id,IntegerType,false), StructField(balance,DoubleType,false), StructField(account_type_id,IntegerType,false))\n",
       "accountsBalanceTypeDf = [account_id: int, balance: double ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[account_id: int, balance: double ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accountsBalanceTypeRdd = sc.parallelize(Array((11, (152.0, 1)), (22, (3545.3, 2)), (33, (12.5, 1)),\n",
    "                                                  (44, (75.0, 1)), (55, (4853.12, 2)), (66, (47.0, 1))))\n",
    "\n",
    "val accountsBalanceTypeSchema = new StructType(Array(StructField(\"account_id\", IntegerType, false),\n",
    "                                                     StructField(\"balance\", DoubleType, false),\n",
    "                                                     StructField(\"account_type_id\", IntegerType, false)))\n",
    "\n",
    "val accountsBalanceTypeDf = spark.createDataFrame(accountsBalanceTypeRdd.map(x=> Row(x._1, x._2._1, x._2._2)),\n",
    "                                                  accountsBalanceTypeSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11,(152.0,1)), (22,(3545.3,2))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accountsBalanceTypeRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------+\n",
      "|account_id|balance|account_type_id|\n",
      "+----------+-------+---------------+\n",
      "|        11|  152.0|              1|\n",
      "|        22| 3545.3|              2|\n",
      "|        33|   12.5|              1|\n",
      "|        44|   75.0|              1|\n",
      "|        55|4853.12|              2|\n",
      "|        66|   47.0|              1|\n",
      "+----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accountsBalanceTypeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accountsTypeDescriptionRdd = ParallelCollectionRDD[23] at parallelize at <console>:31\n",
       "accountsTypeDescriptionSchema = StructType(StructField(account_type_id,IntegerType,false), StructField(account_description,StringType,false))\n",
       "accountsTypeDescriptionDf = [account_type_id: int, account_description: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[account_type_id: int, account_description: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accountsTypeDescriptionRdd = sc.parallelize(Array((1, \"Basic Account\"), (2, \"Premium Account\")))\n",
    "\n",
    "val accountsTypeDescriptionSchema = new StructType(Array(StructField(\"account_type_id\", IntegerType, false),\n",
    "                                                         StructField(\"account_description\", StringType, false)))\n",
    "\n",
    "val accountsTypeDescriptionDf = spark.createDataFrame(accountsTypeDescriptionRdd.map(x=> Row(x._1, x._2)),\n",
    "                                                      accountsTypeDescriptionSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,Basic Account), (2,Premium Account)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accountsTypeDescriptionRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+\n",
      "|account_type_id|account_description|\n",
      "+---------------+-------------------+\n",
      "|              1|      Basic Account|\n",
      "|              2|    Premium Account|\n",
      "+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accountsTypeDescriptionDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Spark Joins\n",
    "\n",
    "We will start with joins of Key / Value RDDs. We can distinguish `join`, `leftOuterJoin` and `rightOuterJoin` joins.\n",
    "\n",
    "`join()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,((John,11),john@gmail.com)), (3,((Maria,33),maria@gmail.com)), (4,((Peter,44),peter@gmail.com)), (5,((Connor,55),connor@gmail.com))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleAccountsRdd.join(peopleEmailsRdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,John,john@gmail.com), (3,Maria,maria@gmail.com), (4,Peter,peter@gmail.com), (5,Connor,connor@gmail.com)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleAccountsRdd.join(peopleEmailsRdd).map(x => (x._1, x._2._1._1, x._2._2)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`leftOuterJoin()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,John,john@gmail.com), (3,Maria,maria@gmail.com), (4,Peter,peter@gmail.com), (5,Connor,connor@gmail.com)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleAccountsRdd.join(peopleEmailsRdd).map(x => (x._1, x._2._1._1, x._2._2)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up join processes, specially if one same RDD is going to be joined several times, it is useful to pre-partition the RDDs.\n",
    "\n",
    "Let's check if the `peopleAccountsRdd` has any partitioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "println(peopleAccountsRdd.partitioner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instantiate a partitioner, and use it to partition two RDDs that will bte joined several times afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partitioner = org.apache.spark.HashPartitioner@2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.HashPartitioner@2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val partitioner = new HashPartitioner(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleAccountsPar = ShuffledRDD[41] at partitionBy at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[41] at partitionBy at <console>:34"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleAccountsPar = peopleAccountsRdd.partitionBy(partitioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some(org.apache.spark.HashPartitioner@2)\n"
     ]
    }
   ],
   "source": [
    "println(peopleAccountsPar.partitioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleEmailsPar = ShuffledRDD[42] at partitionBy at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[42] at partitionBy at <console>:34"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleEmailsPar = peopleAccountsRdd.partitionBy(partitioner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform 10 different joins without any kind of pre-partition on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join time (ms): 97\n",
      "Join time (ms): 79\n",
      "Join time (ms): 59\n",
      "Join time (ms): 74\n",
      "Join time (ms): 72\n",
      "Join time (ms): 79\n",
      "Join time (ms): 78\n",
      "Join time (ms): 97\n",
      "Join time (ms): 106\n",
      "Join time (ms): 123\n"
     ]
    }
   ],
   "source": [
    "for(idx <- 1 to 10) {\n",
    "    val iniTime = System.currentTimeMillis()\n",
    "    peopleAccountsRdd.join(peopleEmailsRdd).collect()\n",
    "    val finTime = System.currentTimeMillis()\n",
    "    println(\"Join time (ms): \" + (finTime - iniTime))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join time (ms): 90\n",
      "Join time (ms): 39\n",
      "Join time (ms): 47\n",
      "Join time (ms): 59\n",
      "Join time (ms): 44\n",
      "Join time (ms): 54\n",
      "Join time (ms): 38\n",
      "Join time (ms): 47\n",
      "Join time (ms): 60\n",
      "Join time (ms): 44\n"
     ]
    }
   ],
   "source": [
    "for(idx <- 1 to 10) {\n",
    "    val iniTime = System.currentTimeMillis()\n",
    "    peopleAccountsPar.join(peopleEmailsPar).collect()\n",
    "    val finTime = System.currentTimeMillis()\n",
    "    println(\"Join time (ms): \" + (finTime - iniTime))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all the times are more or less the same. Now, we repeat the process but using pre-partition RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when joining large RDDs with small RDDs, it is quiet convinient to \"broadcast\" the small RDDs to all the executors. Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bigRdd = MapPartitionsRDD[103] at map at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[103] at map at <console>:32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bigRdd = accountsBalanceTypeRdd.map(x => (x._2._2, (x._1, x._2._1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,(11,152.0)), (2,(22,3545.3)), (1,(33,12.5)), (1,(44,75.0)), (2,(55,4853.12)), (1,(66,47.0))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smallRddLocal = Map(2 -> Premium Account, 1 -> Basic Account)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(2 -> Premium Account, 1 -> Basic Account)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val smallRddLocal = accountsTypeDescriptionRdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(2 -> Premium Account, 1 -> Basic Account)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallRddLocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smallRddLocalBcast = Broadcast(74)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(74)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val smallRddLocalBcast = sc.broadcast(smallRddLocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,((11,152.0),Basic Account)), (2,((22,3545.3),Premium Account)), (1,((33,12.5),Basic Account)), (1,((44,75.0),Basic Account)), (2,((55,4853.12),Premium Account)), (1,((66,47.0),Basic Account))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigRdd.mapPartitions(iter => iter.flatMap{\n",
    "    case(k, v1) => {\n",
    "        smallRddLocalBcast.value.get(k) match {\n",
    "            case Some(v2) => Seq((k, (v1, v2)))\n",
    "            case None => Seq((k, (v1, null)))\n",
    "            }\n",
    "        }   \n",
    "    }\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL \n",
    "\n",
    "Joining DataFrames using the SQL API is quiet simple and efficient. We can highlight the following joining modes: `inner`, `left_outer`, `right_outer`, `outer`, `left_semi` and `left_anti`. Let's see some examples of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----------------+\n",
      "| id|  Name|account_id|           Email|\n",
      "+---+------+----------+----------------+\n",
      "|  1|  John|        11|  john@gmail.com|\n",
      "|  3| Maria|        33| maria@gmail.com|\n",
      "|  5|Connor|        55|connor@gmail.com|\n",
      "|  4| Peter|        44| peter@gmail.com|\n",
      "+---+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleAccountsDf.join(peopleEmailsDf, Seq(\"id\"), \"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`left_outer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+----------------+\n",
      "| id|    Name|account_id|           Email|\n",
      "+---+--------+----------+----------------+\n",
      "|  1|    John|        11|  john@gmail.com|\n",
      "|  6|     Max|        66|            null|\n",
      "|  3|   Maria|        33| maria@gmail.com|\n",
      "|  5|  Connor|        55|connor@gmail.com|\n",
      "|  4|   Peter|        44| peter@gmail.com|\n",
      "|  2|Isabelle|        22|            null|\n",
      "+---+--------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleAccountsDf.join(peopleEmailsDf, Seq(\"id\"), \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rigth_outer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----------------+\n",
      "| id|  Name|account_id|           Email|\n",
      "+---+------+----------+----------------+\n",
      "|  1|  John|        11|  john@gmail.com|\n",
      "|  3| Maria|        33| maria@gmail.com|\n",
      "|  5|Connor|        55|connor@gmail.com|\n",
      "|  4| Peter|        44| peter@gmail.com|\n",
      "+---+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleAccountsDf.join(peopleEmailsDf, Seq(\"id\"), \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`full`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+----------------+\n",
      "| id|    Name|account_id|           Email|\n",
      "+---+--------+----------+----------------+\n",
      "|  1|    John|        11|  john@gmail.com|\n",
      "|  6|     Max|        66|            null|\n",
      "|  3|   Maria|        33| maria@gmail.com|\n",
      "|  5|  Connor|        55|connor@gmail.com|\n",
      "|  4|   Peter|        44| peter@gmail.com|\n",
      "|  2|Isabelle|        22|            null|\n",
      "+---+--------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleAccountsDf.join(peopleEmailsDf, Seq(\"id\"), \"full\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`left_semi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|  Name|account_id|\n",
      "+---+------+----------+\n",
      "|  1|  John|        11|\n",
      "|  3| Maria|        33|\n",
      "|  5|Connor|        55|\n",
      "|  4| Peter|        44|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleAccountsDf.join(peopleEmailsDf, Seq(\"id\"), \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`left_anti`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+\n",
      "| id|    Name|account_id|\n",
      "+---+--------+----------+\n",
      "|  6|     Max|        66|\n",
      "|  2|Isabelle|        22|\n",
      "+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleAccountsDf.join(peopleEmailsDf, Seq(\"id\"), \"left_anti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also make use of broadcast joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{functions=>F}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{functions => F}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-------+-------------------+\n",
      "|account_type_id|account_id|balance|account_description|\n",
      "+---------------+----------+-------+-------------------+\n",
      "|              1|        11|  152.0|      Basic Account|\n",
      "|              2|        22| 3545.3|    Premium Account|\n",
      "|              1|        33|   12.5|      Basic Account|\n",
      "|              1|        44|   75.0|      Basic Account|\n",
      "|              2|        55|4853.12|    Premium Account|\n",
      "|              1|        66|   47.0|      Basic Account|\n",
      "+---------------+----------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accountsBalanceTypeDf.join(F.broadcast(accountsTypeDescriptionDf), \"account_type_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Joins\n",
    "\n",
    "Lastly, we can join also datasets using the `joinWith` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|             _1|                  _2|\n",
      "+---------------+--------------------+\n",
      "|  [1, John, 11]| [1, john@gmail.com]|\n",
      "| [3, Maria, 33]|[3, maria@gmail.com]|\n",
      "|[5, Connor, 55]|[5, connor@gmail....|\n",
      "| [4, Peter, 44]|[4, peter@gmail.com]|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleAccountsDs.joinWith(peopleEmailsDs, peopleAccountsDs(\"id\") === peopleEmailsDs(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
